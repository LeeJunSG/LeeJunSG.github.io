layout single title: "Chapter 8"


## Applying Machine Learning to Sentiment Analysis



감정 분석

- 자연어 처리(NLP)의 하위 필드의 탐구
- 기계 학습 알고리즘을 사용하여 감정, 즉 작성자의 태도를 기반으로 문서를 분류하는 방법
- IMDb(Internet Movie Database)에서 가져온 50,000개의 영화 리뷰 데이터 세트를 사용하여 긍정적인 리뷰와 부정적인 리뷰를 구분할 수 있는 예측 변수를 구축



이 장에서 다룰 주제는 다음과 같다.

- 텍스트 데이터 정리 및 준비

- 텍스트 문서에서 특징 벡터 만들기

- 긍정적인 영화 리뷰와 부정적인 영화 리뷰를 분류하는 머신 러닝 모델 교육

- 핵심 외 학습을 사용하여 대용량 텍스트 데이터 세트 작업

- 분류를 위해 문서 컬렉션에서 주제 추론

​    

​    

## Preparing the IMDb movie review data for text processing

​    

감정 분석(오피니언 마이닝이라고도 함)은 NLP의 광범위한 분야에서 널리 사용되는 하위 분야이다. 

- 문서의 감정을 분석하는 것과 관련이 있음
- 감정 분석에서 인기 있는 작업은 특정 주제에 대해 작성자의 표현된 의견이나 감정을 기반으로 문서를 분류하는 것임



IMDb의 방대한 영화 리뷰 데이터 세트로 작업할 것이다

- 영화 리뷰 데이터 세트는 긍정적이거나 부정적인 레이블이 지정된 50,000개의 극영화 리뷰로 구성됨
- 긍정적인 것은 영화가 IMDb에서 별 6개 이상으로 평가되었음을 의미함
- 부정적이면 영화가 IMDb에서 별 5개 미만으로 평가되었음을 의미함



목표 : 데이터 세트를 다운로드하고 머신 러닝 도구에 사용할 수 있는 형식으로 전처리하고 이러한 영화 리뷰의 하위 집합에서 의미 있는 정보를 추출하여 특정 리뷰어가 좋아하는지 싫어하는지 예측할 수 있는 머신 러닝 모델의 구축



## Preprocessing the movie dataset into a more convenient format



데이터 세트를 성공적으로 추출했으면 이제 압축 해제된 다운로드 아카이브의 개별 텍스트 문서를 단일 CSV 파일로 조합합니다. 다음 코드 섹션에서는 표준 데스크톱 컴퓨터에서 최대 10분이 소요될 수 있는 pandas DataFrame 개체로 영화 리뷰를 읽을 것이다

- 완료까지의 진행 상황과 예상 시간을 시각화하기 위해  Python Progress Indicator(PyPrind, https://pypi.python.org/pypi/PyPrind/) 패키지를 사용
- PyPrind는 pip install pyprind 명령을 실행하여 설치할 수 있음

​    ![그림](/image/image-20221105221452180.png)

- 앞의 코드에서 먼저 읽을 문서의 수인 50,000번의 반복으로 새로운 진행률 표시줄 객체 pbar를 초기화함
- aclImdb 디렉토리로 이동하고 정수 클래스 레이블(1 = 양수 및 0 = 음수)과 함께 결국 df pandas DataFrame에 추가한 pos 및 neg 하위 디렉토리에서 개별 텍스트 파일을 읽음
- 조립된 데이터세트의 클래스 레이블이 정렬되었으므로 이제 np.random 하위 모듈의 순열 함수를 사용하여 DataFrame을 섞음
  -  데이터세트를 이후 섹션에서 훈련 및 테스트 데이터세트로 분할하는 데 유용함

로컬 드라이브에서 직접 데이터를 스트리밍할 때. 우리의 편의를 위해 조합되고 섞인 영화 리뷰 데이터 세트도 CSV 파일로 저장한다.

![그림](/image/image-20221105222826755.png)

저장된 csv파일의 결과는 다음과 같이 저장된다.

![그림](/image/image-20221105222852036.png)

​    

## Transforming words into feature vectors

​    

각 문서의 단어 수를 기반으로 단어 모음 모델을 구성하기 위해 scikit-learn에서 구현된 CountVectorizer 클래스를 사용할 수 있다. 

- CountVectorizer는 문서 또는 문장이 될 수 있는 텍스트 데이터 배열을 사용하여 단어 모음 모델을 구성함

​    ![그림](/image/image-20221105223325657.png)

   

CountVectorizer에서 fit_transform 메서드를 호출하여 bag-of-words 모델의 어휘를 구성하고 다음 세 문장을 희소 특징 벡터로 변환했습니다.

- 'The sun is shining'
- 'The weather is sweet'
- 'The sun is shining, the weather is sweet, and one and one is two'
  - 어휘의 내용을 인쇄 출력 

![그림](/image/image-20221105224848591.png)

- 어휘는 고유한 단어를 정수 인덱스에 매핑하는 Python 사전에 저장됨

특징 벡터 출력

![그림](/image/image-20221105225113600.png)

- 특징 벡터의 각 인덱스 위치는 CountVectorizer 어휘에서 사전 항목으로 저장된 정수 값에 해당

  - 인덱스 위치 0의 첫 번째 특징은 마지막 문서에서만 나타나는 단어 'and'의 개수와 유사하고 인덱스 위치 1(문서 벡터의 두 번째 특징)에서 단어 'is'가 발생함

  - 특징 벡터의 이러한 값은 원시 용어 빈도라고 함

  - tf(t, d) - 문서 d에서 용어 t가 발생하는 횟수임

  - bag-of-words 모델에서 문장이나 문서의 단어 또는 용어 순서는 중요하지 않음

  - 특징 벡터에서 용어 빈도가 나타나는 순서는 일반적으로 알파벳순으로 할당되는 어휘 색인에서 파생됨

    

## Assessing word relevancy via term frequency-inverse document frequency

텍스트 데이터를 분석할 때 두 클래스의 여러 문서에 걸쳐 나타나는 단어를 종종 접하게 된다. 

- 자주 발생하는 단어에는 일반적으로 유용하거나 차별적인 정보가 포함되어 있지 않음
-  tf-idf는 용어 빈도와 역 문서 빈도의 곱으로 정의할 수 있음

​    ![그림](/image/image-20221105231043931.png)

- tf(t, d)는 이전 섹션에서 소개한 용어 빈도이고 idf(t, d)는 역 문서 빈도이며 다음과 같이 계산할 수 있다.

![그림](/image/image-20221105231123032.png)

- nd는 총 문서 수이고 df(d, t)는 t라는 용어를 포함하는 문서 수 d이다.
- 분모에 상수 1을 추가하는 것은 선택 사항이며 훈련 예제에서 발생하지 않는 항에 0이 아닌 값을 할당하는 목적으로 사용됨
- 로그는 낮은 문서 빈도에 너무 많은 가중치가 부여되지 않도록 하는 데 사용됨



이전 소절에서 보았듯이 'is'라는 단어는 세 번째 문서에서 가장 많이 등장하는 단어로 용어 빈도가 가장 높으나, 동일한 특징 벡터를 tf-idfs로 변환한 후 단어 'is'는 이제 세 번째 문서에서 상대적으로 작은 tf-idf(0.45)와 연관됨.

- 유용한 차별 정보를 포함할 가능성이 낮음

기능 벡터에서 개별 항의 tf-idfs를 수동으로 계산했다면 TfidfTransformer가 이전에 정의한 표준 교과서 방정식과 약간 다르게 tf-idfs를 계산한다.



tf-idfs를 계산하기 전에 원시 용어 빈도를 정규화하는 것이 더 일반적이지만 TfidfTransformer 클래스는 tf-idfs를 직접 정규화합니다. 기본적으로(norm='l2') scikit-learn의 TfidfTransformer는 L2 정규화를 적용합니다. 이 정규화는 비정규화된 특징 벡터 v를 L2-norm으로 나누어 길이가 1인 벡터를 반환한다.

![그림](/image/image-20221105231729532.png)



## Cleaning text data

이전 하위 섹션에서 bag-of-words 모델, 용어 빈도 및 tf-idfs에 대해 배웠습니다. 그러나 bag-of-words 모델을 구축하기 전에 첫 번째 중요한 단계는 원하지 않는 문자를 모두 제거하여 텍스트 데이터를 정리하는 것이다.



   텍스트에는 HTML 마크업과 구두점 및 기타 문자가 아닌 문자가 포함되어 있다. HTML 마크업에는 유용한 의미가 많이 포함되어 있지 않지만 구두점은 특정 NLP 컨텍스트에서 유용한 추가 정보를 나타낼 수 있다. 그러나 이제 단순함을 위해 :)와 같은 이모티콘 문자를 제외한 모든 구두점을 제거한다. 이러한 문자는 감정 분석에 확실히 유용하기 때문이다.



## Processing documents into tokens

영화 리뷰 데이터 세트를 성공적으로 준비한 후에는 텍스트 말뭉치를 개별 요소로 분할하는 방법에 대해 생각할 필요가 있습니다. 문서를 토큰화하는 한 가지 방법은 정리된 문서를 공백 문자로 분할하여 개별 단어로 분할하는 것이다.

​    ![그림](/image/image-20221105232534535.png)

토큰화의 맥락에서 또 다른 유용한 기술은 단어 형태소 분석이다.

- 단어를 어근 형태로 변환하는 프로세스임
- 관련 단어를 동일한 줄기에 매핑할 수 있음 



## Training a logistic regression model for document classification

​    

이 섹션에서는 bag-of-words 모델을 기반으로 영화 리뷰를 긍정적인 리뷰와 부정적인 리뷰로 분류하는 로지스틱 회귀 모델을 학습한다. 먼저 정리된 텍스트 문서의 DataFrame을 교육용 문서 25,000개와 테스트용 문서 25,000개로 나눈다.

​    ![그림](/image/image-20221105233425146.png)

다음으로 GridSearchCV 개체를 사용하여 5중 계층 교차 검증을 사용하는 로지스틱 회귀 모델에 대한 최적의 매개변수 세트를 찾는다.

![그림](/image/image-20221105233543928.png)



로지스틱 회귀 분류기의 경우 상대적으로 큰 데이터 세트에 대해 기본 선택('lbfgs')보다 더 나은 성능을 낼 수 있는 LIBLINEAR 솔버를 사용하고 있다.



앞의 코드를 사용하여 GridSearchCV 개체와 해당 매개변수 그리드를 초기화할 때 특징 벡터의 수와 많은 어휘로 인해 그리드 검색을 계산적으로 상당히 비용이 많이 들 수 있기 때문에 제한된 수의 매개변수 조합으로 제한한다. - 

- 하위 섹션의 CountVectorizer 및 TfidfTransformer를 CountVectorizer와 TfidfTransformer를 결합한 TfidfVectorizer로 교체함 
- param_grid는 두 개의 매개변수 사전으로 구성됨
- 첫 번째 사전에서는 기본 설정(use_idf=True, smooth_idf=True 및 norm='l2')과 함께 TfidfVectorizer를 사용하여 tf-idfs를 계산함
- 두 번째 사전에서는 원시 용어 빈도를 기반으로 모델을 훈련하기 위해 해당 매개변수를 use_idf=False, smooth_idf=False 및 norm=None으로 설정함

로지스틱 회귀 분류기 자체에 대해 패널티 매개변수를 통해 L2 정규화를 사용하여 모델을 훈련하고 역 정규화 매개변수 C에 대한 값 범위를 정의하여 다양한 정규화 강도를 비교함

- 선택적인 연습으로 L1을 추가하는 것이 좋음
-  'clf__penalty': ['l2']를 'clf__penalty': ['l2', 'l1']로 변경하여 매개변수 그리드로 정규화



## Working with bigger data – online algorithms and out- of-core learning

​    

코어 외 학습(out-of-core learning)

-  분류기를 데이터 세트의 더 작은 배치에 증분적으로 피팅하여 큰 데이터 세트로 작업할 수 있음
- scikit-learn에서 SGDClassifier의 partial_fit 함수를 사용하여 로컬 드라이브에서 직접 문서를 스트리밍하고 문서의 작은 미니 배치를 사용하여 로지스틱 회귀 모델을 훈련

 movie_data에서 처리되지 않은 텍스트 데이터를 정리하는 토크나이저 함수를 정의하고, 이 장의 시작 부분에서 생성한 csv 파일을 중지 단어를 제거하면서 단어 토큰으로 분리한다.

![그림](/image/image-20221105235207402.png)

다음으로 한 번에 하나의 문서를 읽고 반환하는 생성기 함수 stream_docs를 정의핸대.

![그림](/image/image-20221105235234922.png)

stream_docs 함수가 올바르게 작동하는지 확인하기 위해 movie_data.csv 파일에서 첫 번째 문서를 읽어보겠습니다. 이 문서는 해당 클래스 레이블과 리뷰 텍스트로 구성된 튜플을 반환해야 합니다.

이제 stream_docs 함수에서 문서 스트림을 가져오고 size 매개변수로 지정된 특정 수의 문서를 반환하는 get_minibatch 함수를 정의한다.

![그림](/image/image-20221105235318785.png)



CountVectorizer는 전체 어휘를 메모리에 보관해야 하기 때문에 핵심 외 학습에 사용할 수 없다. 또한, TfidfVectorizer는 역 문서 빈도를 계산하기 위해 훈련 데이터 세트의 모든 특징 벡터를 메모리에 보관해야 한다. 그러나 scikit-learn에서 구현된 텍스트 처리를 위한 또 다른 유용한 벡터라이저는 HashingVectorizer이다. HashingVectorizer는 데이터 독립적이며 Austin Appleby의 32비트 MurmurHash3 기능을 통해 해싱 트릭을 사용한다.

![그림](/image/image-20221105235345486.png)

앞의 코드를 사용하여 토크나이저 함수로 HashingVectorizer를 초기화하고 기능 수를 2**21로 설정했습니다. 또한 SGDClassifier의 loss 매개변수를 'log'로 설정하여 로지스틱 회귀 분류기를 다시 초기화했습니다. HashingVectorizer에서 많은 기능을 선택하면 해시 충돌이 발생할 가능성이 줄어들지만 로지스틱 회귀 모델의 계수 수도 증가한다.

모든 보완 기능을 설정하고 다음 코드를 사용하여 핵심 외 학습을 시작할 수 있다.

![그림](/image/image-20221105235535720.png)



PyPrind 패키지를 사용하여 학습 알고리즘의 진행 상황을 추정한다.

- 진행률 표시줄 개체를 45회 반복하여 초기화함
- 다음 for 루프에서 각 미니 배치가 1,000개의 문서로 구성된 문서의 45개 이상의 미니 배치를 반복함
- 점진적 학습 프로세스를 완료한 후 마지막 5,000개 문서를 사용하여 모델의 성능을 평가함

![그림](/image/image-20221105235557222.png)

모델의 정확도는 약 87%로 초매개변수 조정을 위한 그리드 검색을 사용하여 이전 섹션에서 달성한 정확도보다 약간 낮다. 

​    

## Decomposing text documents with LDA

​    

LDA 이면의 수학은 상당히 복잡하고 베이지안 추론에 대한 지식이 필요하므로 실무자의 관점에서 이 주제에 접근하고 평신도 용어를 사용하여 LDA를 해석한다.

- LDA는 서로 다른 문서에서 자주 함께 나타나는 단어 그룹을 찾으려는 생성 확률 모델
- 자주 나타나는 단어는 각 문서가 다른 단어의 혼합이라고 가정할 때 주제를 나타낸다. 
- LDA에 대한 입력은 이 장의 앞부분에서 논의한 bag-of-words 모델이다.

​    

