layout single
title: "Chapter 5"
typora-copy-images-to: //..\images\2022-10-16-chapter5

## 주제

감독되지 않은 데이터 압축을 위한 주성분 분석

클래스 분리성을 최대화하기 위한 지도 차원 축소 기법으로서의 선형 판별 분석

데이터 시각화를 위한 비선형 차원 축소 기술 및 t-분산 확률적 이웃 임베딩에 대한 간략한 개요

## Unsupervised dimensionality reduction via principal component analysis!

### 주성분 분석을 통한 비지도 차원 축소

다른 특징 추출 기술을 사용하여 데이터 세트의 특징 수를 줄일 수 있음. 

특성 선택과 특성 추출의 차이점

순차 역방향 선택과 같은 특성 선택 알고리즘을 사용할 때 원래 특성을 유지

특성 추출을 사용하여 데이터를 새로운 특성 공간으로 변환하거나 투영

차원 축소의 맥락에서 특징 추출은 대부분의 관련 정보를 유지하기 위한 목적으로 데이터 압축에 대한 접근 방식으로 이해 가능하다



## PCA : principal component analysis

PCA는 기능 간의 상관 관계를 기반으로 데이터의 패턴을 식별하는 데 도움이 된다. 

- PCA는 고차원 데이터에서 최대 분산 방향을 찾는 것을 목표로 하고 데이터를 원래 데이터와 같거나 더 적은 차원을 가진 새로운 부분 공간에 투영한다.

  

새 부분 공간의 직교 축(주성분)은 그림 5.1과 같이 새 기능 축이 서로 직교한다는 제약 조건이 주어지면 최대 분산 방향으로 해석될 수 있다.




![그림](/image/EMB0000295c164c.bmp)


차원 축소를 위해 PCA를 사용하는 경우 d×k 차원 변환 행렬 W를 구성하여 훈련 예제 x의 특징 벡터를 원래의 d차원 특징 공간. 예를 들어, 프로세스는 다음과 같습니다. 특징 벡터 x가 있다고 가정한다.





![그림](/image/image-20221016145654517.png)

## PCA의 4단계

•1. 데이터 표준화

•2. 공분산 행렬 구성

•3. 공분산 행렬의 고유값과 고유벡터 구하기

•4. 고유 벡터의 순위를 매기기 위해 내림차순으로 고유값 정렬

![그림](/image/image-20221016145800316.png)



• 1 단계 : 데이터 표준화

• 데이터 전처리에서 작업한 Wine 데이터 세트를 로드

• 데이터의 70%와 30%를 사용하여 Wine 데이터를 별도의 교육 및 테스트 데이터 세트로 처리하고 단위 분산으로 표준화



•2단계 : 공분산 행렬의 구성

•대칭 d×d-차원 공분산 행렬은 서로 다른 기능 간의 공분산을 저장

•두 특성 x_j와 x_k 간의 공분산

![그림](/image/image-20221016145846206.png)

•"μ" _j  와 "μ" _k은 각각 특성 j와 k의 표본 평균

•두 특성 간의 양의 공분산은 특성이 함께 증가하거나 감소함을 나타냄

•음의 공분산은 특성이 반대 방향으로 변한다는 것을 나타냄

![그림](/image/image-20221016145859144.png)

3 단계 : 공분산 행렬의 고유쌍 구하기

고유 벡터 v는 다음 조건을 충족함
![그림](/image/image-20221016145918880.png)

𝜆는 고유값이다.



![그림](/image/image-20221016145925553.png)



고유 벡터와 고유값의 계산 : NumPy의 linalg.eig 함수를 사용하여 Wine 공분산 행렬의 고유 쌍을 얻을 수 있음

numpy.cov 함수를 사용하여 표준화된 훈련 데이터 세트의 공분산 행렬을 계산한다,\.

 linalg.eig 함수를 사용하여 고유 분해를 수행하여 13개의 고유값으로 구성된 벡터(eigen_vals)와 13×13차원 행렬(eigen_vecs)의 열로 저장된 해당 고유 벡터를 생성한다.



##  Feature transformation

•Wine 데이터 세트를 새로운 주성분 축으로 변환하는 마지막 세 단계

1.k개의 가장 큰 고유값에 해당하는 k개의 고유벡터를 선택합니다. 여기서 k는 새로운 특징 부분공간(k≤d)의 차원

2."상단" k 고유벡터로부터 투영 행렬 W를 구성

3.투영 행렬 W를 사용하여 d차원 입력 데이터 세트 X를 변환하여 새로운 k차원 특징 부분 공간을 얻음

![그림](/image/image-20221016150033615.png)

다음으로, 이 데이터 세트에서 분산의 약 60%를 캡처하기 위해 두 개의 가장 큰 고유값에 해당하는 두 개의 고유 벡터를 수집하고 2차원 산점도를 통해 데이터를 그린다.

​	- 두 개의 고유 벡터를 선택을 선택한다.

​	- 실제로 주성분의 수는 계산 효율성과 분류기의 성능 간의 균형에 의해 결정되야 됨.



앞의 코드를 실행하여 상위 두 개의 고유 벡터에서 13×2 차원 투영 행렬 W이 생성한다.



투영 행렬을 사용하여 이제 예제 x(13차원 행 벡터)를 PCA 부분 공간(주성분 1과 2)으로 변환하여 x'를 얻을 수 있음. (x'=xW)

![그림](/image/image-20221016150153876.png)



•마찬가지로 행렬 내적을 계산하여 전체 124×13차원 훈련 데이터 세트를 두 가지 주요 구성 요소로 변환한다. (X′=XW)

•마지막으로 2차원 산점도에서 124×2차원 행렬로 저장된 변환된 Wine 훈련 데이터 세트를 시각화

•PCA는 클래스 레이블 정보를 사용하지 않는 비지도 기법이라는 점을 명심해야 함

![그림](/image/image-20221016150346729.png)

![그림](/image/image-20221016150350756.png)



## scikit-learn에서 구현된 PCA 클래스를 사용하는 방법

•PCA 클래스는 scikit-learn의 변환기 클래스 중 하나이다.

•동일한 모델 매개변수를 사용하여 훈련 데이터와 테스트 데이터 세트를 모두 변환하기 전에 먼저 훈련 데이터를 사용하여 모델을 피팅한다.

•Wine 교육 데이터 세트에서 scikit-learn의 PCA 클래스를 사용하여,  로지스틱 회귀를 통해 변환된 예제를 분류하고 결정 영역을 시각화한다.

![그림](/image/image-20221016150450728.png)



•plot_decision_regions 코드를 현재 작업 디렉토리의 별도 코드 파일에 배치하고 현재 Python 세션으로 가져올 수 있음.



•훈련 데이터의 결정 영역이 두 개의 주성분 축으로 축소됨.

![그림](/image/image-20221016150511629.png)

![그림](/image/image-20221016150515275.png)

•scikit-learn을 통한 PCA 투영을 자체 PCA 구현과 비교할 때 결과 플롯이 서로의 mirror image임.



•이것은 두 구현 중 하나의 오류가 아니다.



•해당 이유는 고유 solver에 따라 고유 벡터가 음수 또는 양수 부호를 가질 수 있기 때문으로 고유 벡터는 일반적으로 단위 길이 1로 조정됩니다.

• 완전성을 위해 변환된 테스트 데이터 세트에 로지스틱 회귀의 결정 영역을 플롯하여 클래스를 잘 분리할 수 있는지 확인하며, 일반적으로 테스트 데이터 세트의 몇 가지 예만 잘못 분류된다.

![그림](/image/image-20221016150757842.png)

![그림](/image/image-20221016150801875.png)

## Assessing feature contributions



•이 섹션에서는 기본 구성 요소에 대한 원래 기능의 기여도를 평가하는 방법



• PCA를 통해 기능의 선형 조합을 나타내는 주요 구성 요소를 제작



•각 원래 기능이 주어진 주성분에 얼마나 기여도 확인.



• 이러한 기여를 로딩이라고 함. factor loadings는 고유값의 제곱근으로 고유벡터를 스케일링하여 계산할 수 있음



•결과 값은 원래 기능과 주성분 간의 상관 관계 해석



•첫 번째 주성분에 대한 하중



•고유 벡터의 고유값의 제곱근:

![그림](/image/image-20221016151343467.png)

•그런 다음 이 행렬의 첫 번째 열인 첫 번째 주성분인 loadings[:, 0]에 대한 하중을 플로팅



•자체 PCA 구현에 대한 요소 로딩을 계산



•유사한 방식으로 피팅된 scikit-learn PCA 개체에서 로딩을 얻을 수 있음.



• components_는 고유 벡터를 나타냄



• pca.explained_variance_는 고유 값을 나타냅니다.



![그림](/image/image-20221016151403777.png)

![그림](/image/image-20221016151407417.png)

![그림](/image/image-20221016151411444.png)



## Supervised data compression via linear discriminant analysis



•Principal component analysis versus linear discriminant analysis



•PCA와 LDA는 모두 데이터 세트의 차원 수를 줄이는 데 사용할 수 있는 선형 변환 기술임.

•LDA는 비정규화 모델에서 과대적합 정도를 줄이고 계산 효율성을 높이기 위한 특징 추출 기법으로 사용할 수 있음

• PCA는 감독되지 않은 알고리즘이고 LDA는 감독됨.

• LDA는 PCA에 비해 분류 작업에 대한 우수한 기능 추출 기술이라고 생각할 수 있습니다.

• PCA를 통한 전처리가 특정 경우에 이미지 인식 작업에서 더 나은 분류 결과를 가져오는 경향이 있음



•축(LD 1)에 표시된 선형 판별식은 두 개의 정규 분포 클래스를 구분함.



•y축(LD 2)에 선형 판별식이 데이터 세트의 많은 분산을 캡처하지만 클래스 판별 정보를 캡처하지 않기 때문에 좋은 선형 판별식으로 실패임



•LDA의 한 가지 가정은 데이터가 정규 분포를 따르는 것

•클래스가 동일한 공분산 행렬을 가지며 훈련 예제가 통계적으로 서로 독립적이라고 가정합니다.

•가정 중 하나 이상의 가정이 (약간) 위반되더라도 차원 축소를 위한 LDA는 여전히 합리적으로 잘 작동할 수 있음.



![그림](/image/image-20221016151626895.png)



## LDA를 수행하는 데 필요한 주요 단계



1.d차원 데이터셋을 표준화합니다(d는 특징의 수).

2.각 클래스에 대해 d차원 평균 벡터를 계산

3.클래스 간 산포 행렬 S_B와 클래스 내 산포 행렬 S_W  를 구성

4.행렬의 고유 벡터와 해당 고유값 계산 :  S_w^(-1) S_n

5.고유값을 내림차순으로 정렬하여 해당 고유 벡터의 순위 지정

6.d×k 차원 변환 행렬 W를 구성하기 위해 k 최대 고유값에 해당하는 k 고유벡터를 선택

​	-고유 벡터는 이 행렬의 열

7.변환 행렬 W를 사용하여 새 기능 부분 공간에 예제를 투영합니다.



• LDA는 행렬을 고유값과 고유 벡터로 분해한다는 점에서 PCA와 매우 유사

• LDA는 2단계에서 계산된 평균 벡터의 형태로 표현되는 클래스 레이블 정보를 고려함.



## Computing the scatter matrices

•클래스 내 산포 행렬을 구성하는 데 사용할 평균 벡터 계산을 진행



• 클래스 간 분산 행렬. 각 평균 벡터 m_i는 클래스 i 의 예와 관련하여 평균 특성 값 "μ" _"m"  을 저장함

![그림](/image/image-20221016151754377.png)
![그림](/image/image-20221016151800751.png)

•이러한 평균 벡터는 다음 코드로 계산할 수 있음.

•세 레이블 각각에 대해 하나의 평균 벡터를 계산함.

![그림](/image/image-20221016151810528.png)



평균 벡터를 사용하여 이제 클래스 내 산포 행렬 S_W를 계산할 수 있음

![그림](/image/image-20221016151831196.png)



각 개별 클래스 i의 개별 분산 행렬 S_i 를 합산하여 계산함



![그림](/image/image-20221016151834685.png)

산점도 행렬을 계산할 때 가정하는 것은 훈련 데이터 세트의 클래스 레이블이 균일하게 분포되어 있음

![그림](/image/image-20221016151838065.png)

•개별 분산 행렬 S_i를 스케일링한 후 분산 행렬 S_W 로 합산하기를 원합니다.



• 산포 행렬을 클래스 예제의 수 n_i로 나눌 때 산포 행렬을 계산하는 것이 실제로 공분산 행렬 Σ𝑖을 계산하는 것과 동일함



•공분산 행렬은 산포 행렬의 정규화된 버전 :

![그림](/image/image-20221016151850442.png)

•스케일링된 클래스 내 산포 행렬을 계산하는 코드

![그림](/image/image-20221016151857915.png)

 스케일링된 클래스 내 분산 행렬(또는 공분산 행렬)을 계산한 후 다음 단계로 이동하여 클래스 간 산포 행렬 SB 계산



![그림](/image/image-20221016151905210.png)





M은 모든 c 클래스의 예를 포함하여 계산된 전체 평균

![그림](/image/image-20221016151908504.png)



## scikit-learn에 구현된 LDA 클래스

•로지스틱 회귀 분류기가 LDA 변환 후 저차원 훈련 데이터 세트를 처리하는 방법

•정규화 강도를 낮추면 로지스틱 회귀 모델이 훈련 데이터 세트의 모든 예를 올바르게 분류하도록 결정 경계를 이동할 수 있음.

![그림](/image/image-20221016151923581.png)

![그림](/image/image-20221016151926860.png)



## 비선형 차원 축소 고려 이유

• 일반적인 기계 학습 알고리즘은 입력 데이터의 선형 분리 가능성을 가정.

![그림](/image/image-20221016151955619.png)

• 실제 응용 프로그램에서 자주 접할 수 있는 비선형 문제를 처리하는 경우 PCA 및 LDA와 같은 차원 축소를 위한 선형 변환 기술이 최선의 선택이 아닐 수 있음



•비선형 차원 축소 기술의 개발 및 적용 : 매니폴드 학습

• 매니폴드는 고차원 공간에 포함된 저차원 토폴로지 공간을 나타냄.



•매니폴드 학습을 위한 알고리즘은 데이터 포인트 간의 관계가 유지되는 저차원 공간에 데이터를 투영하기 위해 데이터의 복잡한 구조를 캡처해야 합



•비선형 차원 축소 및 매니폴드 학습 알고리즘은 매우 강력하지만 이러한 기술은 사용하기가 매우 어렵고 이상적이지 않은 하이퍼파라미터를 선택하면 득보다 실이 더 많을 수 있음

![그림](/image/image-20221016152000944.png)



## Visualizing data via t-distributed stochastic neighbor embedding



• t-SNE는 고차원(원래) 공간에서 쌍별 거리를 기반으로 데이터 포인트를 모델링함



• 원래 공간에서 쌍별 거리의 확률 분포에 가까운 새로운 저차원 공간에서 쌍별 거리의 확률 분포를 찾음

•즉, t-SNE는 데이터 포인트를 원래 공간의 쌍별 거리가 유지되도록 저차원 공간에 삽입하는 방법을 학습함



•t-SNE는 투영을 위해 전체 데이터 세트가 필요하기 때문에 시각화 목적으로 고안된 기술입니다. 

•포인트를 직접 투영하기 때문에 t-SNE를 새 데이터 포인트에 적용할 수 없음



•. 총 1,797개의 이미지로 구성된 데이터세트의 처음 4개 이미지를 플로팅

• 숫자는 8×8 그레이스케일 이미지


![그림](/image/image-20221016152021739.png)

![그림](/image/image-20221016152027103.png)

•Digits.data 속성을 사용하면 예제가 행으로 표시됨



•열이 픽셀에 해당하는 이 데이터 세트의 테이블 형식 버전에 액세스할 수 있음



•새로운 변수 X_digits에 기능을 할당하고 또 다른 새로운 변수 y_digits에 레이블을 할당함



•그런 다음 scikit-learn에서 t-SNE 클래스를 가져오고 새 tsne 개체를 맞춤.



•fit_transform을 사용하여 t-SNE 피팅 및 데이터 변환을 한 단계로 수행합니다.

![그림](/image/image-20221016152041619.png)

• 64차원 데이터 세트를 2차원 공간에 투영



• init='pca'를 지정 



• t-SNE와 UMAP 모두에서 권장되는 PCA를 사용하여 t-SNE 임베딩을 초기화함.

•2D t-SNE 임베딩을 시각화



•PCA와 마찬가지로 t-SNE는 비지도 방법



• 함수 색상 인수를 통해 시각화 목적으로만 클래스 레이블 y_digits(0-9)를 사용



• Matplotlib의 PathEffects는 시각적 목적으로 사용되어 클래스 레이블이 각 숫자에 속하는 데이터 포인트의 중앙(np.median을 통해)에 표시

• 64차원 데이터 세트를 2차원 공간에 투영



• init='pca'를 지정 



• t-SNE와 UMAP 모두에서 권장되는 PCA를 사용하여 t-SNE 임베딩을 초기화함.

•2D t-SNE 임베딩을 시각화



•PCA와 마찬가지로 t-SNE는 비지도 방법



• 함수 색상 인수를 통해 시각화 목적으로만 클래스 레이블 y_digits(0-9)를 사용



• Matplotlib의 PathEffects는 시각적 목적으로 사용되어 클래스 레이블이 각 숫자에 속하는 데이터 포인트의 중앙(np.median을 통해)에 표시한다



![그림](/image/image-20221016152101410.png)
