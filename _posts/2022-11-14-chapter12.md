layout single title: "Chapter 12"


## Parallelizing Neural Network Training with PyTorch

기계 학습 및 딥 러닝의 수학적 기초에서 PyTorch에 중점을 둔다. PyTorch는 현재 사용 가능한 가장 인기 있는 딥 러닝 라이브러리 중 하나이며 이전 NumPy 구현보다 훨씬 더 효율적으로 신경망(NN)을 구현할 수 있다. 이 장에서  PyTorch를 사용하기 시작하고 이것이 어떻게 훈련 성능에 상당한 이점을 가져오는지 본다.
이 장에서는 머신 러닝과 딥 러닝에서 다음과 같은 주제에대해 설명을 진행한다.

- PyTorch가 훈련 성과를 개선하는 방법

- PyTorch의 Dataset 및 DataLoader를 사용하여 입력 파이프라인을 구축하고 효율적인 모델 교육을 가능하게 합니다.
- PyTorch와 협력하여 최적화된 기계 학습 코드 작성
- 일반적인 딥 러닝 아키텍처를 편리하게 구현하기 위해 torch.nn 모듈을 사용합니다.
- 인공 신경망에 대한 활성화 함수 선택

  

### Performance challenges

컴퓨터 프로세서의 성능은 최근 몇 년 동안 지속적으로 향상되었다. 이를 통해 더 강력하고 복잡한 학습 시스템을 훈련할 수 있으므로 기계 학습 모델의 예측 성능을 향상시킬 수 있다. 

현재 사용 가능한 가장 저렴한 데스크탑 컴퓨터 하드웨어조차도 다중 코어가 있는 처리 장치와 함께 제공된다.

기본적으로 Python은 전역 인터프리터 잠금(GIL)으로 인해 하나의 코어에서 실행하도록 제한된다. 따라서 Python의 다중 처리 라이브러리를 활용하여 여러 코어에 계산을 분산하지만 가장 진보된 데스크탑 하드웨어에는 이러한 코어가 8개 또는 16개 이상 있는 경우가 거의 없다.

다층 인공 신경망 구현에서 처음부터 100개 단위로 구성된 단 하나의 은닉층으로 매우 간단한 다층 퍼셉트론(MLP)을 구현한 된다.

매우 간단한 이미지 분류 작업을 위해 약 80,000개의 가중치 매개변수([784*100 + 100] + [100 * 10] + 10 = 79,510)를 최적화해야 한다다. MNIST의 이미지는 다소 작으며(28×28), 은닉층을 추가하거나 픽셀 밀도가 더 높은 이미지로 작업하려는 경우에만 매개변수 수가 폭발적으로 증가한다. 

이 문제에 대한 확실한 해결책은 실제 작업 장치인 GPU(그래픽 처리 장치)를 사용하는 것이다. 그래픽 카드는 컴퓨터 내부의 작은 컴퓨터 클러스터로 생각할 수 있다. 또 다른 장점은 다음 개요에서 볼 수 있듯이 최신 GPU가 최첨단 중앙 처리 장치(CPU)에 비해 큰 가치가 있다.

### What is PyTorch?

PyTorch는 딥 러닝을 위한 편리한 래퍼를 포함하여 기계 학습 알고리즘을 구현하고 실행하기 위한 확장 가능한 다중 플랫폼 프로그래밍 인터페이스이다. 

머신 러닝 모델 훈련의 성능을 향상시키기 위해 PyTorch는 CPU, GPU 및 TPU와 같은 XLA 장치에서 실행할 수 있다. 그러나 GPU 및 XLA 장치를 사용할 때 가장 뛰어난 성능을 발견할 수 있다.

PyTorch는 공식적으로 CUDA 지원 및 ROCm GPU를 지원한다. 

PyTorch는 노드 집합으로 구성된 계산 그래프를 중심으로 구축된다. 각 노드는 0개 이상의 입력 또는 출력을 가질 수 있는 작업을 나타낸다. PyTorch는 작업을 평가하고, 계산을 실행하고, 구체적인 값을 즉시 반환하는 명령형 프로그래밍 환경을 제공한다. 따라서 PyTorch의 계산 그래프는 미리 구성하고 나중에 실행하는 것이 아니라 묵시적으로 정의된다다.
수학적으로 텐서는 스칼라, 벡터, 행렬 등의 일반화로 이해될 수 있습니다. 보다 구체적으로, 스칼라는 0순위 텐서로, 벡터는 1순위 텐서로, 행렬은 2순위 텐서로, 3차원에 쌓인 행렬은 다음과 같이 정의할 수 있다. 3순위 텐서. PyTorch의 텐서는 자동 미분에 최적화되어 있고 GPU에서 실행할 수 있다는 점을 제외하고 NumPy의 어레이와 유사하다.
텐서의 개념을 더 명확하게 하기 위해 첫 번째 행에 순위 0과 1의 텐서를 나타내고 두 번째 행에 순위 2와 3의 텐서를 나타내는 그림은 다음과 같이 표현이 가능하다.

​    ![그림](/image/image-20221113033335406.png)



### Building input pipelines in PyTorch

심층 신경망 모델을 훈련할 때 우리는 일반적으로 이전 장에서 본 것처럼 확률적 경사하강법과 같은 반복 최적화 알고리즘을 사용하여 모델을 점진적으로 훈련한다.
torch.nn은 NN 모델을 빌드하기 위한 모듈이다. 훈련 데이터 세트가 다소 작고 메모리에 텐서로 로드할 수 있는 경우 이 텐서를 훈련에 직접 사용할 수 있다. 그러나 일반적인 사용 사례에서 데이터 세트가 너무 커서 컴퓨터 메모리에 맞지 않으면 주 저장 장치에서 데이터를 청크로 로드해야 한다. 

 특정 변환 및 사전 처리 단계를 적용하기 위해 데이터 처리 파이프라인을 구성해야 할 수도 있다. 스케일링 또는 노이즈 추가와 같은 데이터를 사용하여 훈련 절차를 보강하고 과적합을 방지한다.
PyTorch는 효율적이고 편리한 전처리 파이프라인을 구성하기 위한 특수 클래스를 제공한다. 



### Combining two tensors into a joint dataset

두 개(또는 그 이상)의 텐서에 데이터를 가질 수 있다.

​	- ex) 기능에 대한 텐서와 레이블에 대한 텐서를 가질 때 : 텐서를 결합하는 데이터 세트를 구축해야 튜플에서 이러한 텐서의 요소를 검색할 수 있다.

### Shuffle, batch, and repeat

분류를 위한 단순 기계 학습 알고리즘 훈련에서 언급했듯이 확률적 경사하강법 최적화를 사용하여 NN 모델을 훈련할 때 훈련 데이터를 무작위로 섞인 배치로 공급하는 것이 중요하다. 데이터 로더 객체의 batch_size 인수를 사용하여 배치 크기를 지정하는 방법을 이미 보았습니다. 이제 배치를 만드는 것 외에도 데이터 세트를 섞고 반복하는 방법을 볼 수 있다.



### The PyTorch neural network module (torch.nn)

torch.nn은 NN을 만들고 훈련하는 데 도움이 되도록 개발된 우아하게 설계된 모듈이다. 몇 줄의 코드로 쉽게 프로토타입을 만들고 복잡한 모델을 구축할 수 있다.

모듈의 기능을 최대한 활용하고 문제에 맞게 사용자 정의하려면 모듈이 수행하는 작업을 이해해야 한다. 이러한 이해를 발전시키기 위해 먼저 torch.nn 모듈의 기능을 사용하지 않고 장난감 데이터 세트에 대한 기본 선형 회귀 모델을 학습한다.
그런 다음, torch.nn 및 torch.optim의 기능을 점진적으로 추가합니다. 다음 하위 섹션에서 볼 수 있듯이 이러한 모듈을 사용하면 NN 모델을 매우 쉽게 구축할 수 있다. 또한 이전 섹션에서 학습한 Dataset 및 DataLoader와 같이 PyTorch에서 지원되는 데이터 세트 파이프라인 기능을 활용할 것입니다. 이 책에서 우리는 NN 모델을 구축하기 위해 torch.nn 모듈을 사용할 수 있다.
PyTorch에서 NN을 구축하기 위해 가장 일반적으로 사용되는 접근 방식은 nn.Module을 사용하는 것인데, 이를 통해 레이어를 쌓아 네트워크를 형성할 수 있다. 이것은 우리에게 포워드 패스에 대한 더 많은 제어를 제공합니다. nn.Module 클래스를 사용하여 NN 모델을 구축하는 예를 살펴보겠습니다.
마지막으로 다음 하위 섹션에서 볼 수 있듯이 학습된 모델을 저장하고 나중에 사용할 수 있도록 다시 로드할 수 있다.

### Building a linear regression model

선형 회귀 문제를 해결하는 간단한 모델

-  먼저 NumPy에서 장난감 데이터 세트를 만들고 시각화
-  모델의 가중치 매개변수를 학습하기 위해 확률적 경사하강법을 사용
-  확률적 경사 하강 절차를 통해 이 교육을 직접 구현하지만 다음 하위 섹션에서는 최적화 패키지인 torch.optim의 SGD 방법을 사용하여 동일한 작업을 수행
- 확률적 기울기 하강 알고리즘을 구현하려면 기울기를 계산해야 한다 
- 이제 학습률을 설정하고 200 Epoch 동안 모델을 훈련할 수 있습니다. 데이터 세트의 일괄 처리 버전에 대해 모델을 학습시키면 다음과 같다.

![그림](/image/image-20221113033710344.png)



### Estimating class probabilities in multiclass classification via the softmax function

 argmax 함수를 사용하여 클래스 레이블을 얻는 방법을 알때, 특정 샘플의 확률은 분모의 정규화 항, 즉 지수 가중 선형 함수의 합으로 계산할 수 있다.



로지스틱 함수에 비해 쌍곡선 탄젠트의 장점은 개방 구간(-1, 1) 범위의 더 넓은 출력 스펙트럼을 가지므로 역전파 알고리즘의 수렴을 향상시킬 수 있다.
대조적으로, 로지스틱 함수는 열린 간격(0, 1) 범위의 출력 신호를 반환한다. 로지스틱 함수와 쌍곡선 탄젠트의 간단한 비교를 위해 두 개의 S자형 함수를 플로팅했을 때, 다음과 같이 나타난다.





![그림](/image/image-20221113033918620.png)

### Broadening the output spectrum using a hyperbolic tangent

인공 신경망의 은닉층에서 자주 사용되는 또 다른 S자형 함수는 쌍곡선 탄젠트(일반적으로 tanh로 알려짐)이며, 이는 로지스틱 함수의 크기 조정 버전으로 해석될 수 있다.

### Rectified linear unit activation

ReLU(Rectified Linear Unit)는 Deep NN에서 자주 사용되는 또 다른 활성화 함수이다. ReLU에 대해 알아보기 전에 한 걸음 물러나서 tanh 및 로지스틱 활성화의 소멸 기울기 문제를 이해해야 한다.

- 처음에 순 입력 z1 = 20이 있다고 가정하고 z2 = 25로 변경합니다. tanh 활성화를 계산하면 순 입력에 대한 활성화의 도함수가 z가 커질수록 감소한다는 것을 확인 가능하다. 결과적으로, 기울기 항이 0에 매우 가까울 수 있기 때문에 훈련 단계에서 가중치를 학습하는 것은 매우 느려진다. ReLU 활성화는 이 문제를 해결하며, 수학적으로 ReLU는 다음과 같이 정의된다.

![그림](/image/image-20221113035031515.png)

