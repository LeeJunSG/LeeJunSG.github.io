layout single title: "Chapter 16"


16장


### Attention helps RNNs with accessing information

주의 메커니즘의 개발을 이해하려면 그림과 같이 번역을 생성하기 전에 전체 입력 시퀀스(예: 하나 이상의 문장)를 구문 분석하는 언어 번역과 같은 seq2seq 작업에 대한 전통적인 RNN 모델은 다음과 같다.



![그림](/image/image-20221127020337530.png)



첫 번째 출력을 생성하기 전에 RNN이 전체 입력 문장을 구문 분석하는 이유는 그림에 설명된 것처럼 문장을 한 단어씩 번역하는 것이 문법적 오류를 초래할 가능성이 높다는 사실 때문입니다.



![그림](/image/image-20221127020432366.png)



이 seq2seq 접근법의 한 가지 한계는 RNN이 변환하기 전에 하나의 숨겨진 장치를 통해 전체 입력 시퀀스를 기억하려고 하는 점이다. 이 때문에 모든 정보를 하나의 hidden layer로 압축하면 정보의 손실 가능성이 존재하기 때문이다.  따라서 인간이 문장을 번역하는 방법과 유사하게 각 시간 단계에서 전체 입력 시퀀스에 액세스하는 것이 좋다.

일반 RNN과 달리 주의 메커니즘을 통해 RNN은 주어진 각 시간 단계에서 모든 입력 요소에 액세스할 수 있다.

- 각 시간 단계에서 모든 입력 시퀀스 요소에 액세스하는 것은 부담스러울 수 있다. 

- RNN이 입력 시퀀스의 가장 관련성이 높은 요소에 초점을 맞출 수 있도록 주의 메커니즘은 각 입력 요소에 서로 다른 주의 가중치를 할당한다. 

- 주의 가중치는 지정된 시간 단계에서 지정된 입력 시퀀스 요소의 중요성 또는 관련성을 지정한다. 



### The original attention mechanism for RNNs

attention mechanism이 있는 RNN(앞에서 언급한 논문을 본떠 모델링)은 두 번째 출력 단어를 생성하는 전반적인 개념은 다음 그림과 같다.


![그림](/image/image-20221127020821896.png)

주의 메커니즘은 각 요소에 가중치를 할당하고 모델이 입력의 어느 부분에 집중해야 하는지 식별하는 데 도움을 줍니다.  위의 그림에 표시된 주의 메커니즘이 있는 RNN은 두 번째 출력 단어를 생성하는 전체 개념을 확인할 수 있다.

그림의 주의 기반 RNN의  RNN #1은 컨텍스트 벡터를 생성하는 양방향 RNN이다. 컨텍스트 벡터는 입력 벡터의 증강된 버전으로 생각할 수 있다. 즉, 입력 벡터는 주의 메커니즘을 통해 다른 모든 입력 요소의 정보도 통합한다. 그림에서 볼 수 있듯이, RNN #2는 RNN #1에 의해 준비된 이 컨텍스트 벡터를 사용하여 출력을 생성한다.

양방향 RNN #1은 입력 시퀀스 x를 정규 순방향으로 처리한다.역방향과 함께 사용합니다. 시퀀스를 역방향으로 구문 분석하는 것은 원래 입력 시퀀스를 역순으로 읽는 것과 같은 효과가 있다. 현재 입력이 문장의 앞이나 뒤에 오는 시퀀스 요소 또는 둘 다에 의존할 수 있기 때문에 이에 대한 근거는 추가 정보를 캡처하는 것이다.
결과적으로, 입력 시퀀스를 두 번(즉, 앞으로 및 뒤로) 읽음으로써, 우리는 각 입력 시퀀스 요소에 대해 두 개의 숨겨진 상태를 갖게 된다. 예를 들어, 두 번째 입력 시퀀스 요소 𝑥(2)의 경우, 우리는 전진 패스에서 숨겨진 상태 ℎ(2)를 얻고 후진 패스에서 숨겨진 상태 ℎ(2)를 얻는다. 이 두 개의 숨겨진 상태는 숨겨진 상태를 형성하기 위해 연결된다. 



### Starting with a basic form of self-attention

자가 주의 작업 뒤에 있는 세 가지 주요 단계를 요약=

1. 주어진 입력 요소인 𝒙(𝑖)와 집합 {1, ..., T}의 각 j번째 요소에 대해 점을 계산한다.
2. Softmax 기능을 사용하여 도트 제품을 정규화하여 주의 가중치를 구한다.
3. 출력 𝒛(i)를 전체 입력 시퀀스의 가중치 합으로 계산한다.

![그림](/image/image-20221127031512805.png)





### Parameterizing the self-attention mechanism: scaled dot-product attention

변압기 아키텍처에 사용되는 스케일 도트 제품 주의라고 하는 보다 진보된 자기 주의 메커니즘을 요약한다.

자기 주의 메커니즘을 보다 유연하고 모델 최적화에 맞게 조정하기 위해 모델 훈련 중에 모델 매개 변수로 적합할 수 있는 세 가지 추가 가중치 행렬이 있다. 이 세 가지 무게 행렬을 𝑼(q), 𝑼(k), 𝑼(v)로 나타내며, 다음과 같이 쿼리, 키 및 값 시퀀스 요소에 입력을 투영하는 데 사용한다.



그림은 두 번째 입력 요소에 해당하는 상황 인식 전자 침구 벡터를 계산하기 위해 이러한 개별 구성 요소가 어떻게 사용되는지 보여준다.

![그림](/image/image-20221127031603174.png)



### Attention is all we need: introducing the original transformer architecture

원래 트랜스포머 아키텍처는 RNN에서 처음 사용된 주의 메커니즘을 기반으로 한다. 원래 주의 메커니즘을 사용한 목적은 긴 문장으로 작업할 때 RNN의 텍스트 생성 기능을 향상시키는 것이다. 하지만, RNN에 대한 주의 메커니즘을 실험한 지 불과 몇 년 후, 연구원들은 주의 기반 언어 모델이 반복되는 계층을 삭제했을 때 훨씬 더 강력하다는 것을 발견했다.  자기 주의 메커니즘 덕분에 트랜스포머 모델은 NLP 컨텍스트에서 입력 시퀀스의 요소 간 장거리 종속성을 캡처할 수 있다. 

이 변압기 아키텍처는 원래 언어 번역을 위해 설계되었지만, 영어 구성 요소 구문 분석, 텍스트 생성 및 텍스트 분류와 같은 다른 작업으로 일반화될 수 있다.  변압기 용지에서 수정한 그림은 주요 아키텍처와 구성 요소를 보여준다.



![그림](/image/image-20221127031650300.png)



### Encoding context embeddings via multi-head attention

8개의 주의 헤드 위에 있는 1차원 인덱스와 입력 문장과 유사한 컨텍스트 벡터는 16차원 벡터이다. 

그런 다음, 이 벡터들을 하나의 긴 길이 벡터로 연결하고 선형 투영을 사용하여 다시 길이 벡터로 매핑합니다. 이 과정은 그림에 나와 있다.

![그림](/image/image-20221128170521684.png)



### Learning a language model: decoder and masked multi-head attention



인코더와 마찬가지로 디코더에도 여러 반복 레이어가 포함되어 있다. 마스킹 주의는 원래 주의 메커니즘의 변형으로, 마스킹 주의는 특정 수의 단어를 "마스킹"하여 제한된 입력 시퀀스만 모델로 전달한다. 다른 모든 단어는 모델이 "부정행위"를 하지 않도록 모델에서 숨겨진다. 또한, 텍스트 생성의 본질과 일치한다. 훈련 중에 진정한 번역된 단어가 알려져 있지만, 실제적으로 기본적인 진실에 대해 아무것도 모른다. 따라서 위치 i에서 모델이 이미 생성한 것에 대한 솔루션만 제공할 수 있다.
그림은 디코더 블록에서 레이어가 배열되는 방법을 보여준다.

![그림](/image/image-20221127031810168.png)



### Pre-training and fine-tuning transformer models

반면, 미세 조정 접근법은 역전파를 통해 정기적으로 감독되는 방식으로 사전 훈련된 모델 매개 변수를 업데이트한다. 기능 기반 방법과 달리, 우리는 또한 분류와 같은 특정 작업을 수행하기 위해 사전 훈련된 모델에 완전히 연결된 다른 레이어를 추가한 다음 레이블이 지정된 훈련 세트의 예측 성능을 기반으로 전체 모델을 업데이트한다. 이 접근법을 따르는 인기 있는 모델 중 하나는 양방향 언어 모델로 사전 훈련된 대규모 변압기 모델인 BERT이다. 다음 그림은 트랜스포머 모델을 교육하는 두 단계를 요약하고 기능 기반 접근법과 미세 조정 접근법의 차이를 보여준다.

![그림](/image/image-20221127031904997.png)

### Leveraging unlabeled data with GPT

2018년에 출시된 GPT-1 모델의 교육 절차는 두 단계로 나눌 수 있다.

1. 레이블이 지정되지 않은 많은 양의 일반 텍스트에 대한 사전 교육을 한다.
2. 미세 조정을 감독한다.

그림에서 알 수 있듯이, GPT-1을 (1) 디코더(및 인코더 블록 없음) 및 (2) 특정 작업을 수행하기 위해 감독된 미세 조정을 위해 나중에 추가되는 추가 레이어로 구성된 변압기로 간주할 수 있다.

![그림](/image/image-20221127031938553.png)



그림은 제로샷, 원샷, 퓨샷 및 미세 조정 절차의 차이를 보여준다.

![그림](/image/image-20221127032014321.png)



### Bidirectional pre-training with BERT

BERT는 언어 이해를 위한 심층 양방향 변압기 사전 교육을 통해 개발했다. GPT와 BERT는 서로 다른 아키텍처이기 때문에 직접 비교할 수는 없지만 BERT는 3억 4천 5백만 개의 매개 변수를 가지고 있다.

이름에서 알 수 있듯이, BERT는 양방향 교육 절차를 활용하는 변압기-인코더 기반 모델 구조를 가지고 있다. 이 설정에서 특정 단어의 인코딩은 앞의 단어와 뒤의 단어 모두에 따라 달라진다. GPT에서 입력 요소는 강력한 생성 언어 모델을 형성하는 데 도움이 되는 자연스러운 왼쪽에서 오른쪽 순서로 읽힌다. 양방향 교육은 단어별로 문장을 생성하는 BERT의 기능을 비활성화하지만 모델이 이제 양방향으로 정보를 처리할 수 있기 때문에 분류와 같은 다른 작업에 더 높은 품질의 입력 인코딩을 제공한다.
트랜스포머의 인코더에서 토큰 인코딩은 위치 인코딩과 토큰 임베딩의 합이다. BERT 인코더에는 이 토큰이 어느 세그먼트에 속하는지 나타내는 추가 세그먼트 임베딩이 있다. 즉, 그림에서 알 수 있듯이 각 토큰 표현에는 세 가지 성분이 포함되어 있다.

![그림](/image/image-20221127032049793.png)


다음 그림은 모델 미세 조정 설정이 매우 간단한 구조를 가지고 있음을 나타낸다. 입력 인코더가 사전 훈련된 BERT에 부착되고 분류를 위해 소프트맥스 레이어가 추가된다. 모델 구조가 설정되면 학습 프로세스에 따라 모든 매개 변수가 조정된다.

![그림](/image/image-20221127032123727.png)
