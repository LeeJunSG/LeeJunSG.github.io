layout single title: "Chapter 15"

15장

## Modeling Sequential Data Using Recurrent Neural Networks

​    

•특수 유형의 순차데이터

•시계열 데이터 : 시간의 차원과 연관됨

•주가와 음성, 음성 기록등

•자연어 처리, 텍스트 모델링 등

•시계열 데이터 아님



•순차적 데이터는 데이터 포인트 간의 순서가 중요며 이를 그림으로 나타내면 다음과 같다.

![그림](/image/image-20221120225256754.png)



•이미지 데이터를 위한 MLP(Multilayer Perceptron) 및 CNN과 같이 지금까지 다룬 표준 NN 모델은 훈련 예제가 서로 독립적이므로 정보를 통합하지 않는다고 가정한다.

•샘플은 피드포워드 및 역전파 단계를 통해 전달되며, 가중치는 교육 예제가 처리되는 순서와 독립적으로 업데이트됩니다



•RNN은 시퀀스를 모델링하도록 설계되었으며 과거 정보를 기억하고 그에 따라 새로운 이벤트를 처리할 수 있으며, 이는 시퀀스 데이터로 작업할 때 분명한 이점이다.



### The different categories of sequence modeling



•퀀스 모델링에는 언어 번역(예: 텍스트를 영어에서 독일어로 번역), 이미지 캡션 및 텍스트 생성과 같은 많은 흥미로운 응용 프로그램이 있습니다. 그러나 적절한 아키텍처와 접근 방식을 선택하려면 이러한 다양한 시퀀스 모델링 작업을 이해하고 구별할 수 있어야 한다.



•순환 신경망의 불합리한 효과에 대한 훌륭한 기사의 설명을 바탕으로 입력 및 출력 데이터의 관계 범주에 따라 달라지는 가장 일반적인 시퀀스 모델링은 다음과 같이 나타낼수 있다.

![그림](/image/image-20221120225541332.png)

RNNs for modeling sequences

이 섹션에서는 PyTorch에서 RNN을 구현하기 전에 RNN의 주요 개념에 대해 설명합니다. 우리는 모델 시퀀스 데이터에 대한 재귀적 구성 요소를 포함하는 RNN의 일반적인 구조를 살펴보는 것으로 시작할 것입니다. 그런 다음, 우리는 일반적인 RNN에서 뉴런 활성화가 어떻게 계산되는지 조사할 것입니다. 이를 통해 RNN 훈련에서 일반적인 과제에 대해 논의할 수 있는 컨텍스트를 만들고 LSTM 및 게이트 반복 단위(GRU)와 같은 이러한 과제에 대한 솔루션에 대해 논의할 것입니다.

​                       

### Understanding the dataflow in RNNs

입력 계층(x), 숨겨진 계층(h) 및 출력 계층(o)이 많은 단위를 포함하는 벡터로 RNN의 데이터 흐름을 나타내면 다음과 같다.



![그림](/image/image-20221120225639790.png)





표준 피드포워드 네트워크의 정보

-입력에서 숨겨진 계층으로, 그리고 숨겨진 계층에서 출력 계층으로 흐릅 



RNN에서 숨겨진 레이어는 현재 시간 스텝의 입력 레이어와 이전 시간 스텝의 숨겨진 레이어로부터 입력을 받음

 숨겨진 계층의 인접 시간 단계의 정보 흐름을 통해 네트워크는 과거 이벤트의 메모리를 가질 수 있다.

이 때, RNN은 두개 이상의 hidden layer를 가질 수 있는데, 이는 다음과 같이 나타낼 수 있다.



![그림](/image/image-20221120225759620.png)



### Computing activations in an RNN



•하나의 숨겨진 레이어를 고려 : 동일한 개념이 다층 RNN에 적용했을 때, RNN은 

![그림](/image/image-20221120225836426.png)

RNN 표현의 각 방향 에지(상자 사이의 연결)는 가중치 행렬과 연관되어 있다. 

이러한 가중치는 시간 t에 의존하지 않으므로 시간 축 전체에서 공유된다.



•특정 구현에서는 가중치 행렬인 W(xh)와 W(hh)가 결합된 행렬 Wh = [Wxh; Whh]에 연결되어 있는 것을 관찰할 수 있음. 

•활성화를 계산하는 것은 표준 다층 퍼셉트론 및 다른 유형의 피드포워드 NN과 매우 유사

• 숨겨진 레이어의 경우, 순 입력 h(사전 활성화)는 선형 결합을 통해 계산됩니다. 즉, 해당 벡터를 사용하여 가중치 행렬의 곱셈의 합을 계산하고 편향 단위를 추가하며, 이렇게  활성화를 계산하는 프로세스는 다음과 같이 나타낸다.

![그림](/image/image-20221120225936360.png)

Hidden recurrence versus output recurrence



•반복 연결이 출력 계층에서 발생하는 대체 모델이 있습니다. 이 경우 이전 시간 단계인 ot–1에서 출력 계층의 순 활성화를 다음 두 가지 방법 중 하나로 추가할 수 있다. 현재 시간 단계에서 숨겨진 레이어, h(t)로 이동한다. 아키텍처 간의 차이는 반복 링 연결에서 명확하게 확인할 수 있다. 표기법에 따라, 반복 연결과 관련된 가중치는 Whh에 의해 숨겨진 반복에 대해, Woh에 의해 숨겨진 반복에 대한 요약은 다음과 같다.

![그림](/image/image-20221120230053442.png)



### The challenges of learning long-range interactions



앞서 간단히 언급한 BPTT는 몇 가지 새로운 과제를 소개합니다. 다중 유발 요인 때문에 손실 함수의 그레이디언트를 계산할 때 소위 소멸 및 폭발 그레이디언트 문제가 발생합니다.

이러한 문제는 그림 15.8의 예로 설명되며, 단순성을 위해 숨겨진 장치가 하나만 있는 RNN을 보여줍니다.



![그림](/image/image-20221118165156818.png)



### Long short-term memory cells

다중 유발 요인 때문에 손실 함수의 그레이디언트를 계산할 때 소위 소멸 및 폭발 그레이디언트 문제가 발생한다.

•|w| < 1이면 t – k가 크면 이 인자는 매우 작아집니다. 반면에, 만약 반복되는 가장자리의 무게가 |w| > 1이라면, t – k가 클 때 wt–k는 매우 커집니다. 큰 t – k는 장거리 종속성을 나타냅니다. |w| = 1을 보장함으로써 기울기가 사라지거나 폭발하는 것을 피하는 순진한 해결책에 도달할 수 있음을 알 수 있다.

![그림](/image/image-20221120230213957.png)

•LSTM은 메모리 셀을 사용하여 장거리 의존성을 모델링하는 동안 기울기 문제가 사라지는데 기여한다.

![그림](/image/image-20221120230220382.png)

LSTM은 사라지는 기울기 문제(S에 의한 장기 단기 메모리)를 극복하기 위해 처음 도입되었다. LSTM의 구성 요소는 메모리 셀이며, 기본적으로 표준 RNN의 숨겨진 계층을 나타내거나 대체합니다.각 메모리 셀에는 우리가 논의한 것처럼 사라지고 폭발하는 기울기 문제를 극복하기 위해 바람직한 무게 w = 1을 갖는 반복 에지가 있다. 이 반복 에지와 관련된 값을 집합적으로 셀 상태라고 한다.



### Project one – predicting the sentiment of IMDb movie reviews

다대일 아키텍처를 사용하여 감정 분석을 위한 다층 RNN 코드는 다음과 같다.

![그림](/image/image-20221120230532910.png)

•각각의 고유한 단어를 고유한 정수에 매핑은 다음과 같으며 

![그림](/image/image-20221120230633069.png)


![그림](/image/image-20221120230640860.png)



매핑에 대한 설명을 나타내는 그림으로 표현하면 다음과 같다.

![그림](/image/image-20221120230701499.png)

단어 인덱스는 여러 가지 방법으로 입력 기능을 지수를 0과 1의 벡터로 변환하기 위해 원핫 인코딩을 적용하는 것은 단어를 전체 데이터 세트의 고유 단어 수가 크기인 벡터에 매핑한다. 고유한 단어의 수(어휘의 크기)가 104 – 105의 순서일 수 있다는 점을 고려할 때, 훈련된 모델은 차원성의 저주로 어려움을 겪을 수 있는 문제가 있다.



임베딩 이용 아이디어 접근법은 각 단어를 실제 값 요소가 있는 고정된 크기의 벡터에 매핑하는 것으로 원핫 인코딩 벡터와 대조적으로, 유한한 크기의 벡터를 사용하여 무한한 수의 실수를 나타낼 수 있다. 임베딩은 여기서 우리가 데이터 세트의 단어를 표현하기 위해 주요 기능을 자동으로 학습하기 위해 사용할 수 있는 기능 학습을 지원한다. 



첫 번째 배치를 사용하여 개별 요소의 크기를 인쇄한 후 이를 미니 프레임으로 결합하고 결과 미니 프레임의 치수를 표시하는 코드는 다음과 같으며,

![그림](/image/image-20221120231229217.png)

위의 코드의 결과로 다음과 같이 나타난다.

![그림](/image/image-20221120231257520.png)

 배치의 다른 세 가지 예제(각각 길이가 165, 86 및 145)는 이 크기와 일치하는 데 필요한 만큼 패딩되는 것이 확인된다.  마지막으로, 3개의 데이터 세트를 배치 크기가 32인 데이터 로더로 나눈다.



위의 데이터 세트들을 가지고 rnn을 사용하여 모듈 클래스, 임베딩 계층, RNN의 반복 계층 및 완전히 연결된 비재귀 계층을 결합할 수 있는데, 이를 코드로서 나타내면 다음과 같다. 

![그림](/image/image-20221120231403562.png)

반복 계층을 하고 싶다면 코드를 다음과 같이 수정하여 구현 할 수 있다.

![그림](/image/image-20221120231515881.png)

 LSTM 레이어를 사용하며, 임베딩 레이어를 사용한 감정 분석을 위한 RNN 모델 생성하고, 유형 LSTM의 반복 레이어가 추가, 1연결된 계층을 숨겨진 계층으로 추가, 완전히 연결된 계층을 출력 계층을 예측과 같이 로지스틱 S 시그모이드 활성화를 통해 단일 클래스 멤버쉽 확률 값을 반환하면 다음과 같이 출력된다.

![그림](/image/image-20221120231637584.png)

주어진 데이터 세트에서 모델을 훈련시키고 분류 정확도와 손실을 반환하는 열차 기능을 개발하고, 주어진 데이터 세트에서 모델의 성능을 측정하는 평가 기능을 개발, 손실 함수와 최적화 도구(Adam optimizer) 제작, 단일 클래스 멤버쉽 확률 출력이 있는 이진 분류의 경우 이진 교차 엔트로피 손실(BCELoss)을 손실 함수로 적용하고 자 하면 다음과 같이 할 수 있다.

![그림](/image/image-20221120231846216.png)

10개 EPISODE에 대한 모델을 교육하는 코드로는 다음과 같이 나타낼 수 있다.

![그림](/image/image-20221120231901811.png)

LSTM의 양방향 구성을 True로 설정하여, 반복 레이어가 양방향, 시작에서 끝까지 그리고 역방향의 입력 시퀀스를 통과하게 하고, 양방향 RNN 계층은 각 입력 시퀀스에 대해 전진 패스와 후진 패스의 두 가지 패스를 만든다. 이때,  전진 및 후진 패스의 결과의 숨겨진 상태는 일반적으로 단일 숨겨진 상태로 연결된다. 다른 병합 모드에는 합계, 곱셈 및 평균화를 표현하면 다음과 같다.

![그림](/image/image-20221120232204918.png)

![그림](/image/image-20221120232211467.png)
