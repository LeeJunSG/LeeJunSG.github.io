layout single title: "Chapter 19"


19장





## Reinforcement Learning for Decision Making in Complex Environments



### Understanding reinforcement learning

RL은 종종 "기계 학습의 세 번째 범주"로 간주된다.
RL을 지도 학습 및 비지도 학습과 같은 기계 학습의 다른 하위 작업과 구별하는 핵심 요소는 RL이 상호 작용에 의한 학습 개념을 중심으로 한다는 것이다. 

- RL에서 모델이 보상 함수를 최대화하기 위해 환경과의 상호 작용에서 학습한다는 것을 의미함
- 보상 함수를 최대화하는 것은 지도 학습에서 손실 함수를 최소화하는 개념과 관련이 있음
- 일련의 동작을 학습하기 위한 올바른 레이블은 RL에서 사전에 알려져 있지 않거나 정의되지 않움
- 특정한 원하는 결과를 달성하기 위해 환경과의 상호 작용을 통해 학습해야 함
- RL을 사용하여 모델(에이전트라고도 함)은 환경과 상호 작용함
- 에피소드라고 하는 일련의 상호 작용을 생성함
- 상호 작용을 통해 에이전트는 환경에 의해 결정된 일련의 보상을 수집함
- 보상은 긍정적이거나 부정적일 수 있으며, 때로는 에피소드가 끝날 때까지 에이전트에게 공개되지 않음



RL에서는 에이전트, 컴퓨터 또는 로봇에게 작업 수행 방법을 가르칠 수 없거나 가르쳐 주지 않는다.

- 에이전트가 달성하기를 원하는 것만 지정할 수 있음
- 특정 시험 결과를 바탕으로 에이전트의 성패에 따라 보상을 결정할 수 있음
- 문제 해결 작업이 알 수 없거나 설명하기 어렵거나 정의하기 어려운 일련의 단계를 필요로 할 때 복잡한 환경에서 RL을 의사 결정에 매우 매력적으로 만듬


RL은 특정 목표를 달성하기 위해 임의의 일련의 행동을 학습하기 위한 강력한 프레임워크를 제공하지만, RL은 여전히 해결되지 않은 많은 과제가 있다.

- 훈련 RL 모델을 특히 어렵게 만드는 한 가지 측면은 결과 모델 입력이 이전에 수행된 작업에 따라 다름

  -  보통 불안정한 학습 행동을 초래함

- RL의 시퀀스 의존성은 지연 효과를 생성함

  - 시간 단계 t에서 취한 조치가 임의의 수의 단계 후에 미래 보상이 나타날 수 있음

    

### Defining the agent-environment interface of a reinforcement learning system



RL의 모든 예에서 에이전트와 환경이라는 두 개의 별개의 엔티티를 찾을 수 있다. 

- 공식적으로 에이전트는 결정을 내리는 방법을 배우고 조치를 취함으로써 주변 환경과 상호 작용하는 엔티티로 정의됨

- 조치를 취한 결과, 에이전트는 환경에 의해 통제되는 관찰과 보상 신호를 받음

-  환경은 에이전트 외부에 있는 모든 것임

- 환경은 에이전트와 통신하고 에이전트의 작업 및 관찰에 대한 보상 신호를 결정함

- 보상 신호는 에이전트가 환경과 상호 작용하여 받는 피드백으로, 일반적으로 스칼라 값의 형태로 제공되며 양수 또는 음수일 수 있음

- 보상의 목적은 에이전트에게 얼마나 잘 수행되었는지를 알려주는 것 

- 에이전트가 보상을 받는 빈도는 지정된 작업 또는 문제에 따라 달라짐

  

그림 19.1은 에이전트와 환경 간의 상호 작용 및 통신을 보여준다.



![그림](/image/image-20221204163737359.png)

위의 그림과 같이 에이전트의 상태(1)는 모든 변수의 집합이다. 

각 시간 단계에서 에이전트는 (2)에서 사용 가능한 작업 집합을 통해 환경과 상호 작용한다. 상태 S(t)에 있는 동안 A(t)로 표시된 에이전트가 수행한 작업을 기준으로 에이전트는 보상 신호 R(t+1)(3)을 수신하고 상태는 S(t+1)(4)가 된다.



### Markov decision processes

일반적으로 RL이 다루는 문제의 유형은 일반적으로 마르코프 의사결정 프로세스(MDP)로 공식화된다. MDP 문제를 해결하기 위한 표준 접근법은 동적 프로그래밍을 사용하는 것이지만 RL은 동적 프로그래밍에 비해 몇 가지 주요 이점을 제공한다. 그러나 동적 프로그래밍은 상태의 크기가 상대적으로 클 때 실현 가능한 접근법이 아니다. 이러한 경우, RL은 MDP를 해결하기 위한 훨씬 더 효율적이고 실용적인 대안 접근법으로 간주됩니다.



### The mathematical formulation of Markov decision processes

알려지지 않은 환경 역학을 보상하기 위해 환경과 상호 작용하여 많은 샘플을 획득해야 한다. 

문제를 처리하기 위한 두 가지 주요 접근법

- 모델이 없는 몬테카를로(MC) 
- 시간적 차이(TD) 방법

다음 표는 두 가지 주요 범주와 각 방법의 분기를 표시한다.



![그림](/image/image-20221204163928154.png)



주어진 상태에 대한 특정 동작이 항상 또는 절대로 수행되지 않는 경우 환경 역학은 결정론적인 것으로 간주될 수 있다. 그렇지 않으면, 환경은 확률적인 행동을 가진다.



이 확률적 행동을 이해하기 위해, 현재 상태 S(t=s)와 수행된 동작 A(t=a)에 따라 조건화된 미래 상태 S(t+1) = s'를 관찰할 확률을 고려하면, 다음과 같이 표시된다.

![그림](/image/image-20221204163958580.png)



### Visualization of a Markov process

마르코프 과정은 그래프의 노드가 환경의 다른 상태를 나타내는 방향 순환 그래프로 표현될 수 있다. 그래프의 노드 간 연결은 상태 간의 전환 확률을 나타내며, 이는 다음 그림으로 나타낸 마르코프 과정으로 확인할 수 있다.



![그림](/image/image-20221204164030550.png)



그래프의 가장자리에 있는 값은 학생의 행동에 대한 전이 확률을 나타내며, 해당 값은 오른쪽 표에도 표시된다. 표의 행을 고려할 때 각 상태에서 나오는 전환 확률은 항상 1이다.



### Episodic versus continuing tasks

에이전트가 환경과 상호 작용하면서 관찰 또는 상태의 시퀀스가 궤적을 형성한다. 



궤적의 두 가지 유형

- 에이전트의 궤적이 각각 시간 t = 0에서 시작하여 터미널 상태 ST(t = T)에서 끝나도록 하위 부분으로 분할될 수 있는 경우, 작업을 일시적 작업이라고 함
- 궤도가 터미널 상태 없이 무한히 연속적인 경우, 작업을 연속 작업이라고 함

에피소드 작업에서 에피소드는 에이전트가 시작 상태인 S(0)에서 종료 상태인 S(T)로 이동하는 시퀀스 또는 궤적이다.

다음 그림은 시험을 위해 공부하는 학생의 과제를 묘사 하는 마르코프 과정의 경우, 다음과 같은 세 가지 예와 같은 에피소드를 가진다.

![그림](/image/image-20221204165100804.png)



### RL terminology: return, policy, and value function



#### The return

t에서의 반환이라고 불리는 것은 한 에피소드의 전체 지속 기간으로부터 얻은 누적 보상이다. 

- R(t+1) = r은 행동을 수행한 후 얻은 즉각적인 보상이며, 시간 t에서 후속 보상은 R(t+2), R(t+3) 등 이다.

그런 다음 t 시점의 수익은 다음과 같이 즉시 보상과 후속 보상으로 계산할 수 있다.

![그림](/image/image-20221204165223422.png)

#### Policy

일반적으로 𝜋(𝑎|𝑠)로 표시되는 정책은 다음에 취할 행동을 결정하는 기능으로, 결정론적이거나 확률적일 수 있다. 확률 정책은 주어진 상태에서 에이전트가 취할 수 있는 액션에 대한 확률 분포를 가진다.

![그림](/image/image-20221204165255923.png)



#### Value function

상태-값 함수라고도 하는 값 함수는 각 상태의 선량도를 측정합니다. 선량 기준은 반환을 기준으로 한다.
이제 리턴 G(t)를 기반으로, 다음 정책을 따른 후 상태의 값 함수를 기대 수익률로 정의한다.

![그림](/image/image-20221204165321364.png)



### Dynamic programming using the Bellman equation

벨만 방정식은 많은 RL 알고리즘의 중심 요소 중 하나이다. 벨만 방정식은 값 함수의 계산을 단순화하여 여러 시간 단계에 걸쳐 합산하는 대신 반환 계산을 위한 재귀와 유사한 재귀를 사용한다.

![그림](/image/image-20221204165410995.png)



위와 같은 식을 벨만 방정식이라고 하는데, 이 방정식은 상태 s의 값 함수와 후속 상태 s'의 값 함수를 연관시킨다. 이것은 시간축을 따라 반복되는 루프를 제거하기 때문에 값 함수의 계산을 크게 단순화할 수 있다.



### Reinforcement learning with Monte Carlo

MC 방법을 사용하면 학습 과정은 시뮬레이션 경험에 기초하며, 사용 할 때의 조건은 다음과 같다.

- 환경 역학에 대한 지식이 없다고 가정함

- 환경의 상태 전이 확률을 알지 못함 

- 에이전트가 환경과 상호 작용을 통해 학습하기를 원함 

  

MC 기반 RL의 경우 확률론적 정책인 𝜋를 따르는 에이전트 클래스를 정의하고, 이 정책을 기반으로 에이전트는 각 단계에서 조치를 취한다. 이렇게 하면 시뮬레이션된 에피소드가 발생한다. 이 때, MC 기반 메서드는 에이전트가 환경과 상호 작용하는 시뮬레이션된 에피소드를 생성하여 이 문제를 해결한다. 이러한 시뮬레이션된 에피소드를 통해 시뮬레이션된 에피소드에서 방문한 각 주의 평균 수익률을 계산할 수 있다.



### State-value function estimation using MC

각 상태에 대해 일련의 에피소드를 생성한 후 모든 에피소드가 상태를 통과하는 일련의 에피소드를 고려하여 상태 값을 계산한다. 룩업 테이블을 사용하여 값 함수인 𝑉(𝑆(𝑡) = 𝑠)에 해당하는 값을 구한다고 가정한다. 값 함수를 추정하기 위한 MC 업데이트는 상태를 처음 방문한 이후 해당 에피소드에서 얻은 총 반환값을 기반으로 하며, 이 알고리즘을 첫 번째 방문 몬테카를로 값 예측이라고 한다.



### Action-value function estimation using MC

첫 번째 방문 MC 상태 값 예측을 위한 알고리듬을 확장할 수 있습니다. 하지만 일부 작업을 선택하지 않아 탐색이 부족할 때, 문제가 발생한다. 

문제 해결 방법

- 가장 간단한 접근법 : 탐색적 시작이라고 하며, 모든 상태-행동 쌍이 에피소드 시작 시 0이 아닌 확률을 갖는다고 가정함
- 𝜖-greedy 정책 



### Finding an optimal policy using MC control

MC 제어는 정책을 개선하기 위한 최적화 절차이다.

최적의 정책에 도달할 때까지 정책 평가와 정책 개선을 반복적으로 번갈아 수행할 수 있다. 따라서 임의의 정책인 φ(0)부터 시작하여 정책평가와 정책개선을 번갈아 수행하는 과정을 다음과 같이 표현 할 수 있다.

![그림](/image/image-20221204165856795.png)



### Policy improvement – computing the greedy policy from the action- value function

action- value 함수 q(s, a)가 주어지면 다음과 같이 탐욕스러운 정책을 생성할 수 있다.

![그림](/image/image-20221204165937509.png)



탐색 부족 문제를 방지하고 방문하지 않은 상태-작용 쌍을 고려하기 위해 최적이 아닌 작업을 선택할 수 있는 작은 기회(σ)를 가진다.



### Temporal difference learning

TD 학습은 경험에 의한 학습을 기반으로 하므로 환경 역학 및 전환 확률에 대한 지식이 필요하지 않는다. TD와 MC 기법의 가장 큰 차이점은 MC에서는 에피소드가 끝날 때까지 기다려야 총 수익률을 계산할 수 있다는 것이다.
그러나 TD 학습에서는 학습된 속성 중 일부를 활용하여 에피소드가 끝나기 전에 추정된 값을 업데이트할 수 있다. 이를 부트스트랩이라고 한다.



### TD prediction

가치 예측으로 각 에피소드가 끝날 때마다 각 시간 단계 t에 대한 수익 G(t)를 추정할 수 있다. 따라서 방문한 주에 대한 추정치를 다음과 같이 업데이트할 수 있다.

![그림](/image/image-20221204170041718.png)

여기서 Gt는 추정치를 업데이트하기 위한 목표 반환값으로 사용되며, (G(t) – V(S(t))는 현재 추정치 V(S(t))에 추가된 보정 용어이다. 𝛼는 학습 속도를 나타내는 매개 변수로, 학습 중에 일정하게 유지된다.



### On-policy TD control (SARSA)

온폴리시 TD 제어 알고리듬은 n단계 TD로 쉽게 일반화할 수 있으며, 각 상태-동작 쌍에 대한 동작-값 함수를 나타내는 표 형식의 2D 배열인 Q(St, At)를 사용하여 다음과 같이 나타낼 수 있다.

![그림](/image/image-20221204172254426.png)

이 알고리즘은 업데이트 공식에 사용되는 5중(S(t), A(t), R(t+1), S(t+1), A(t+1))로 인하여 SARSA라고 불린다.
GPI 프레임워크를 사용할 수 있으며, 랜덤 정책을 시작으로 현재 정책에 대한 액션-값 함수를 반복적으로 추정한 후 현재 액션-값 함수를 기반으로 하는 γ-탐욕 정책을 사용하여 정책을 최적화할 수 있다.



### Off-policy TD control (Q-learning)

에이전트가 정책 𝜋를 준수하여 현재 전환 5중주(S(t), A(t), R(t+1), S(t+1), A(t+1))로 에피소드를 생성한다고 가정 할 때, 에이전트가 수행하는 작업 값인 A(t+1)을 사용하여 작업 값 함수를 업데이트하는 대신, 현재 정책에 따라 에이전트가 선택하지 않더라도 최상의 작업을 찾을 수 있다. 이 때, 업데이트 규칙을 수정하여 다음 즉시 상태에서 서로 다른 작업을 변경하여 최대 Q-값을 고려할 수 있다. Q 값을 업데이트하기 위한 수정 방정식은 다음과 같다.

![그림](/image/image-20221204172326145.png)

다음 상태인 S(t+1)에서 최상의 작업을 찾고 수정 항에서 이를 사용하여 Q(St, At)에 대한 추정치를 업데이트한다.



### A glance at deep Q-learning

때때로 상태의 수가 매우 커질 수 있으며, 이는 거의 무한대로 커질 수 있다. 또한, 이산 상태로 작업하는 대신 연속 상태 공간을 다룰 수 있다. 더욱이, 일부 상태는 훈련 중에 전혀 방문되지 않을 수 있으며, 이는 나중에 이러한 보이지 않는 상태를 처리하기 위해 에이전트를 일반화할 때 문제가 될 수 있다.



이러한 문제를 해결하기 위해 V(S(t)) 또는 Q(S(t), A(t))와 같은 표 형식으로 값 함수를 나타내는 대신 동작 값 함수에 대해 함수 근사 접근법을 사용한다. 여기서, 우리는 참 값 함수의 근사치를 학습할 수 있는 매개 변수 함수 v(w)(x(s))를 정의한다. 여기서 x(s)는 입력 기능의 집합이다.
근사 함수 q(w)(x(s), a)가 심층 신경망(DNN)인 경우, 결과 모델을 심층 Q-네트워크(DQN)라고 한다. DQN 모델을 훈련하기 위해 가중치는 Q-러닝 알고리듬에 따라 업데이트된다. DQN 모델의 예는 다음 그림과 같다. 여기서 상태는 첫 번째 계층에 전달된 기능으로 표시된다.

![그림](/image/image-20221204173441221.png)



### Replay memory

Q-러닝에 대한 이전 표 형식 방법을 사용하면 다른 사람의 값에 영향을 미치지 않고 특정 상태-행동 쌍에 대한 값을 업데이트할 수 있다. 그러나 이제 NN 모델로 q(s, a)를 근사하기 때문에 상태-동작 쌍에 대한 가중치를 업데이트하면 다른 상태의 출력에도 영향을 미칠 가능성이 높다. 감독 작업에 대해 확률적 경사 하강법을 사용하여 NN을 훈련할 때, 여러 에포크를 사용하여 훈련 데이터가 수렴될 때까지 여러 번 반복한다.



 Q-러닝에서 실현 불가능 문제 원인

- 훈련 중에 에피소드가 변경되고, 결과적으로 훈련 초기에 방문했던 일부 주들은 나중에 방문할 가능성이 줄어들 것이기 때문임

- 다른 문제는 NN을 훈련할 때 훈련 예제가 독립적이고 동일하게 분포된다라고 가정한다는 것임

- 에이전트의 에피소드에서 추출된 샘플은 일련의 전환을 형성하기 때문에 독립적이고 동일하지 않음

  

문제를 해결하기 위해 에이전트가 환경과 상호 작용하고 전이 5중 q(w)(x(s), a)를 생성할 때, 재생 메모리라고 불리는 메모리 버퍼에 많은 전환을 저장한다. 각 새 상호 작용 후에는 결과적으로 새 전환 5중이 메모리에 추가된다.

메모리 크기를 제한적으로 유지하기 위해 가장 오래된 전환이 메모리에서 제거된다. 그런 다음 메모리 버퍼에서 미니 배치 예제를 랜덤하게 선택하여 손실을 계산하고 네트워크 매개 변수를 업데이트하는 데 사용한다. 이는 다음 그림의 프로세스를 통해 확인 할 수 있다.



![그림](/image/image-20221204173516196.png)





### Determining the target values for computing the loss

표 형식의 Q-러닝 방법에서 요구되는 또 다른 변경 사항은 DQN 모델 매개 변수를 훈련하기 위해 업데이트 규칙을 적용하는 방법있다. 

다음 그림처럼, DQN 모델의 전진 패스를 두 번 수행한다. 



![그림](/image/image-20221204173538755.png)



- 첫 번째 전진 패스는 현재 상태(x(s))의 기능을 사용한다. 
- 두 번째 전진 패스는 다음 상태의 기능을 사용합니다. 
- 첫 번째 전진 패스와 두 번째 전진 패스에서 각각 추정 동작 값을 구하게 된다. 

전환 5중주에서, 우리는 작업 a가 에이전트에 의해 선택된다는 것을 알 수 있다. 따라서 Q-러닝 알고리즘에 따라 그에 상응하는 동작 값을 업데이트해야 한다. 따라서, 위의 그림과 같이 다른 동작에 대한 동작 값을 유지하는 목표 동작 값 벡터를 만들 수 있다.

이 때, 다음 세 가지 양을 사용하여 이 문제를 회귀 문제로 처리한다.
• 현재 예측된 값인 𝑞(𝑤) x(s,:)입니다.
• 설명된 목표값 벡터
• 표준 평균 제곱 오차(MSE) 손실 함수

따라서 a를 제외한 모든 작업에 대해 손실이 0이 된다. 마지막으로, 계산된 손실은 네트워크 매개 변수를 업데이트하기 위해 역전파된다.

