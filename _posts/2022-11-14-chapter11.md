layout single title: "Chapter 11"


## Implementing a Multilayer Artificial Neural Network from Scratch

​    

딥 러닝은 여러 계층을 가진 인공 신경망(NN)을 효율적으로 훈련하는 것과 관련된 기계 학습의 하위 분야로 이해될 수 있다.

딥러닝에 대한 설명은 다음과 같은 과정으로 설명한다.

• 다층 신경망에 대한 개념적 이해

• NN 교육을 위한 기본 역전파 알고리즘을 처음부터 구현

• 이미지 분류를 위한 기본 다층 신경망 훈련



##  Introducing the multilayer neural network architecture

​    

다중 단일 뉴런을 다중 레이어 피드포워드 NN에 연결하는 방법으로 특별한 유형의 완전히 연결된 네트워크를 MLP라고도 한다.

​    ![그림](/image/image-20221113003416833.png)

   데이터 입력으로 위의 그림에 묘사된 MLP에는 하나의 은닉 계층과 하나의 출력 계층이 있다.

은닉층의 단위는 입력 피처에 완전히 연결되고 출력층은 은닉층에 완전히 연결된다. 이러한 네트워크에 두 개 이상의 은닉층이 있는 경우 이를 심층 NN이라고도 한다. 또한, 일부 상황에서는 입력도 계층으로 간주된다. 그러나 이 경우 단일 계층 신경망인 Adaline 모델을 2계층 신경망으로 만들어 직관적이지 않을 수 있다.



## Activating a neural network via forward propagation



MLP 학습 절차를 세 가지 단계

1. 입력 계층에서 시작하여 네트워크를 통해 훈련 데이터의 패턴을 전달하여 출력을 생성함

2. 네트워크의 출력을 기반으로 나중에 설명할 손실 함수를 사용하여 최소화하려는 손실을 계산함

3. 손실을 역전파하고 네트워크의 각 가중치 및 편향 단위에 대한 미분을 찾고 모델을 업데이트함

마지막으로, 여러 에포크에 대해 이 세 단계를 반복한다.



 MLP의 가중치 및 편향 매개변수를 학습한 후 순전파를 사용하여 네트워크 출력을 계산하고 임계값 함수를 적용하여 원-핫 표현에서 예측된 클래스 레이블을 얻는다.

​    

## Implementing a multilayer perceptron



MNIST 데이터 세트는 다음 네 부분으로 구성된다.

1. 훈련 데이터 세트 이미지
2. 훈련 데이터 세트 레이블
3. 테스트 데이터 세트 이미지
4. 데이터 세트 레이블 테스트



## Computing the loss function



MSE 손실을 사용하여 그라디언트의 유도를 더 쉽게 따라갈 수 있도록 다층 NN을 훈련한다. 



![그림](/image/image-20221113022124074.png)

이 단순화된 그림에서 W(h)와 W(out) 모두 동일한 수의 은닉 유닛으로 MLP를 초기화하지 않는 한 동일한 수의 행과 열을 갖는 것처럼 보일 수 있다.
이는 역전파 알고리즘의 맥락에서 W(h) 및 W(out)의 차원에 대해 더 자세히 설명 가능하다.

 역전파는 다층 신경망에서 복잡하고 볼록하지 않은 손실 함수의 편도함수를 계산하기 위한 계산적으로 매우 효율적인 접근 방식으로 생각할 수 있다. 



## Training neural networks via backpropagation

역전파 수학을 통해 NN에서 가중치를 매우 효율적으로 학습하는 방법을 이해할 수 있다.  다음과 같이 공식화한 출력 레이어의 활성화를 얻기 위해 먼저 순방향 전파를 적용해야 한다.

​    즉, 두 개의 입력 기능, 세 개의 숨겨진 노드 및 두 개의 출력 노드가 있는 네트워크에 대해 아래의 그림과 같이 화살표로 표시된 것처럼 네트워크의 연결을 통해 입력 기능을 전달한다.





![그림](/image/image-20221113032817476.png)





역전파에서는 오른쪽에서 왼쪽으로 오류를 전파한다. 이것을 모델 가중치(및 편향 단위)에 대한 손실의 기울기를 계산하기 위해 순방향 패스 계산에 체인 규칙을 적용하는 것이다. 단순화를 위해 출력 레이어의 가중치 행렬에서 첫 번째 가중치를 업데이트하는 데 사용되는 편미분에 대한 이 프로세스를 설명가능하다. 역전파하는 계산 경로는 아래 굵은 화살표를 통해 강조 표시하면 다음과 같다.

![그림](/image/image-20221113032842544.png)



은닉층 가중치의 편도함수(또는 기울기) 계산에도 관련되기 때문이다.
은닉층 가중치는 다음 그림과 같이 은닉층의 첫 번째 가중치에 대한 손실의 편미분을 계산하는 방법통해 구할 수 있다.

![그림](/image/image-20221113032907171.png)



## About convergence in neural networks

미니 배치 학습을 사용하여 NN을 훈련 시키는 이유로는 미니 배치 학습이 1 < k < n인 n 훈련 예제의 하위 집합 k를 기반으로 기울기를 계산하는 SGD의 특별한 형태이다. 따라서, 미니 배치 학습은 벡터화된 구현을 사용하여 계산 효율성을 향상시킬 수 있다는 점에서 온라인 학습에 비해 이점이 있다. 그러나 일반 경사 하강법보다 훨씬 빠르게 가중치를 업데이트할 수 있다. 직관적으로 미니 배치 학습은 전체 인구에게 묻는 것이 아니라 인구의 대표 하위 집합에게만 질문하여 여론 조사에서 대통령 선거의 투표율을 예측하는 것으로 생각할 수 있다.

다층 신경망은 Adaline, 로지스틱 회귀 또는 지원 벡터 머신과 같은 단순한 알고리즘보다 훈련하기가 훨씬 어렵다. 다층 신경망에서는 일반적으로 최적화해야 하는 수백, 수천 또는 수십억 개의 가중치가 있는데,  출력 함수의 표면이 거친 최적화 알고리즘은 아래의 그림과 같이 극소값에 쉽게 갇힐 수 있다.



![그림](/image/image-20221113032928785.png)

