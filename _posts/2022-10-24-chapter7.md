layout single
title: "Chapter 7"

## Learning with ensembles

앙상블 방법의 목표는 다른 분류기를 개별 분류기 단독보다 일반화 성능이 더 나은 메타 분류기로 결합하는 것이다.

7장에서는 앙상블이 작동하는 방식과 일반적으로 좋은 일반화 성능을 제공하는 것으로 인식되는 이유에 대한 설명을 진행한다.

7 장에서는 다수결 원칙을 사용하는 앙상블 방법에 초점을 둔다.

- 다수결 투표는 단순히 다수의 분류자가 예측한, 즉 50% 이상의 투표를 받은 클래스 레이블을 선택하는 것을 의미함.
- "과반수 투표"라는 용어는 바이너리 클래스 설정만을 나타냄.
- 다수결 원칙을 다중 투표라고 하는 다중 클래스 설정으로 일반화하는 것은 쉬움.



가장 많은 표를 받은 클래스 레이블(모드)을 선택한다. 그림 10개의 분류기로 구성된 앙상블에 대한 다수결 및 다수결 투표의 개념을 보여준다.

- 각 고유 기호(삼각형, 정사각형 및 원)는 고유한 클래스 레이블을 나타냄.

![그림](/image/image-20221024113725773.png)

훈련 데이터 세트를 사용하여 m개의 서로 다른 분류기(C1, ..., Cm)를 훈련하는 것으로 시작함. 

기술에 따라 결정 트리, 지원 벡터 머신, 로지스틱 회귀 분류기 등과 같은 다양한 분류 알고리즘에서 앙상블을 구축할 수 있음. 

동일한 기본 분류 알고리즘을 사용하여 훈련 데이터 세트의 다른 하위 집합에 맞출 수도 있음. 

이 접근 방식의 한 가지 두드러진 예는 3장, Scikit-Learn을 사용한 기계 학습 분류기 둘러보기에서 다룬 다양한 의사 결정 트리 분류기를 결합한 랜덤 포레스트 알고리즘임. 그림은 과반수 투표를 사용하는 일반 앙상블 접근 방식의 개념을 보여준다.

![그림](/image/image-20221024113741664.png)

## Implementing a simple majority vote classifier

이 섹션에서 구현하려는 알고리즘을 사용하면 신뢰도를 위해 개별 가중치와 관련된 다양한 분류 알고리즘을 결합할 수 있다.

- 목표 : 특정 데이터세트에 대한 개별 분류기의 약점을 보완하는 더 강력한 메타 분류기를 구축하는 것임.

  -  수학적 용어로 가중 다수결을 다음과 같이 작성할 수 있다. ![그림](/image/image-20221024113813021.png)

  - wj는 기본 분류기 Cj와 관련된 가중치임

  -  𝑦^은 앙상블의 예측 클래스 레이블임.

  - A는 고유한 클래스 레이블 집합.

  - 𝜒_𝐴(그리스어 chi)는 특성 함수 또는 표시기 함수로, j번째 분류기의 예측된 클래스가 i(Cj(x) = i)와 일치하면 1을 반환함.

    

## Bagging – building an ensemble of classifiers from bootstrap samples

Bagging은 이전 섹션에서 구현한 MajorityVoteClassifier와 밀접하게 관련된 앙상블 학습 기술이다.

-  앙상블의 개별 분류기에 맞추기 위해 동일한 훈련 데이터 세트를 사용하는 대신 초기 훈련 데이터 세트에서 부트스트랩 샘플(교체된 무작위 샘플)을 가져옴.
-  배깅을 부트스트랩 집계라고도 함.

![그림](/image/image-20221024114024487.png)

## Bagging in a nutshell

배깅 분류기의 부트스트랩 집계가 작동하는 방식에 대한 보다 구체적인 예를 제공하기 위해 그림에 표시된 예를 살펴보겠다. 

- 각 배깅 라운드에서 교체와 함께 무작위로 샘플링되는 7개의 다른 훈련 인스턴스(인덱스 1-7로 표시)가 있다.
- 각 부트스트랩 샘플을 사용하여 가장 일반적으로 정리되지 않은 의사결정 트리인 분류기 Cj에 맞춥니다.
- 각 분류기는 훈련 데이터 세트에서 예제의 임의 하위 집합을 받음. 
- 배깅을 통해 얻은 이러한 무작위 샘플을 배깅 라운드 1, 배깅 라운드 2 등으로 표시함. 
- 각 하위 집합에는 특정 부분의 중복이 포함되어 있으며 일부 원본 예제는 대체 샘플링으로 인해 다시 샘플링된 데이터 집합에 표시되지 않는다.
- 개별 분류기가 부트스트랩 샘플에 적합하면 과반수 투표를 사용하여 예측이 결합된다.
  - 배깅은 3장에서 소개한 랜덤 포레스트 분류기와도 관련이 있다. 사실 랜덤 포레스트는 개별 의사 결정 트리를 맞출 때 랜덤 기능 하위 집합도 사용하는 배깅의 특별한 경우임.

![그림](/image/image-20221024114040631.png)



## How adaptive boosting works

배깅과 대조적으로, 부스팅 알고리즘의 초기 공식은 교체 없이 훈련 데이터 세트에서 가져온 훈련 예제의 무작위 하위 집합을 사용한다.

##### 부스팅 절차의 네 가지 주요 단계

1. 훈련 데이터 세트 D에서 대체하지 않고 훈련 예제 d1의 무작위 부분 집합(샘플)을 그려 약한 학습자 C1을 훈련함.

2. 훈련 데이터 세트에서 대체하지 않고 두 번째 무작위 훈련 하위 집합 d2를 그리고 이전에 잘못 분류된 예제의 50%를 추가하여 약한 학습자 C2를 훈련함.

3. C1과 C2가 동의하지 않는 훈련 데이터 세트 D에서 훈련 예제 d3을 찾아 세 번째 약한 학습자 C3을 훈련함.

4. 과반수 투표를 통해 약한 학습자 C1, C2 및 C3을 결합함.



부스팅은 배깅 모델에 비해 편향과 분산을 감소시킬 수 있다. 

+ AdaBoost와 같은 부스팅 알고리즘은 높은 분산, 즉 훈련 데이터를 과적합하는 경향으로도 알려져 있음.
+ AdaBoost는 전체 교육 데이터 세트를 사용하여 약한 학습자를 교육함
+ 교육 예제는 앙상블에서 이전 약한 학습자의 실수로부터 학습하는 강력한 분류기를 구축하기 위해 각 반복에서 다시 가중치를 부여함.



AdaBoost 알고리즘의 구체적인 세부 사항에 대해 더 자세히 알아보기 전에 그림 7.9를 살펴보겠습니다.

AdaBoost의 기본 개념을 더 잘 이해하려면:

![그림](/image/image-20221024114128500.png)

- AdaBoost 그림을 단계별로 살펴보기 위해 하위 그림 1부터 시작합니다. 이 그림은 모든 훈련 예제에 동일한 가중치가 할당된 이진 분류를 위한 훈련 데이터 세트를 나타냄. 
- 이 훈련 데이터 세트를 기반으로 두 클래스(삼각형 및 원)의 예를 분류하고 손실 함수(또는 특수 결정 트리 앙상블의 경우)를 계산함.
- 다음 라운드(하위 그림 2)에서는 이전에 잘못 분류된 두 가지 예(원)에 더 큰 가중치를 할당함.
- 올바르게 분류된 예제의 가중치를 낮춤. 다음 결정 스텀프는 이제 가중치가 가장 큰 훈련 예제, 즉 분류하기 어려운 훈련 예제에 더 집중함.
- 하위 그림 2에 표시된 약한 학습자는 하위 그림 3과 같이 서클 클래스에서 세 가지 다른 예를 잘못 분류한 다음 더 큰 가중치를 할당함.
- AdaBoost 앙상블이 3 라운드의 부스팅으로만 구성되어 있다고 가정하면, 하위 그림 4와 같이 가중치가 적용된 과반수 투표에 의해 서로 다른 재가중 훈련 하위 집합에 대해 훈련된 3개의 약한 학습자를 결합함.



## Gradient boosting – training an ensemble based on loss gradients

그라디언트 부스팅은 부스팅 개념의 또 다른 변형이다.

- 약한 학습자를 연속적으로 훈련시켜 강력한 앙상블을 만듬. 
- 그래디언트 부스팅은 Kaggle 대회에서 우승한 것으로 잘 알려진 XGBoost와 같은 인기 있는 기계 학습 알고리즘의 기반을 형성하기 때문에 매우 중요한 주제임.



## Comparing AdaBoost with gradient boosting

그래디언트 부스팅은 이 장에서 이전에 논의한 AdaBoost와 매우 유사합니다.

 AdaBoost

- 이전 의사결정나무 그루터기의 오류를 기반으로 하여 의사결정나무 그루터기를 훈련함.
- 오류는 개별 그루터기를 앙상블로 결합할 때 각 결정 트리 그루터기에 대한 분류기 가중치를 계산할 뿐만 아니라 각 라운드의 샘플 가중치를 계산하는 데 사용됨.
- 최대 반복 횟수(의사결정 트리 그루터기)에 도달하면 훈련을 중지함.

 AdaBoost와 마찬가지로 그래디언트 부스팅은 예측 오류를 사용하여 반복적인 방식으로 의사 결정 트리에 적합하다. 

- 그래디언트 부스팅 트리는 일반적으로 결정 트리 그루터기보다 더 깊고 일반적으로 최대 깊이가 3~6(또는 최대 수 8~64 리프 노드)임.
- AdaBoost와 달리 그래디언트 부스팅은 샘플 가중치 할당에 예측 오류를 사용하지 않음.
- 다음 트리를 맞추기 위한 대상 변수를 형성하는 데 직접 사용됨.
- AdaBoost에서와 같이 각 트리에 대해 개별적인 가중치를 부여하는 대신 그래디언트 부스팅은 각 트리에 대해 동일한 전역 학습률을 사용함.

 AdaBoost와 그래디언트 부스팅은 몇 가지 유사점을 공유하지만 특정 핵심 측면에서 다르다. 

 

## Outlining the general gradient boosting algorithm

분류를 위한 그래디언트 부스팅을 살펴보겠습니다. 간단하게 이진 분류 예제를 살펴보겠다.

1. 일정한 예측 값을 반환하도록 모델을 초기화한다.

   - 의사 결정 트리 루트 노드를 사용함.

   - 즉, 단일 리프 노드가 있는 의사결정 트리입니다. 트리에서 반환된 값을 𝑦^ 로 표시하고 나중에 정의할 미분 가능한 손실 함수 L을 최소화하여 이 값을 찾음.

     ![그림](/image/image-20221024173949447.png)

     - 여기서 n은 데이터 세트의 n개의 훈련 예제임

2. 각 트리 m = 1, ..., M(여기서 M은 사용자가 지정한 총 트리 수)에 대해 아래 2a~2d단계에 설명된 다음 계산을 수행한다.

   a. 예측값 𝐹(𝑥𝑖 ) = 𝑦^𝑖 와 클래스 레이블 yi 간의 차이를 계산함. 이 값을 의사 응답 또는 의사 잔차라고도 한다. 더 공식적으로, 우리는 이 의사 잔차를 예측된 값에 대한 손실 함수의 음의 기울기로 쓸 수 있다.

   ![그림](/image/image-20221024174421839.png)

   b. 의사 잔차 테두리에 나무를 맞춥니다. Rjm이라는 표기법을 사용하여 반복 m에서 결과 트리의 j = 1 ... Jm 리프 노드를 나타낸다.

   c. 각 리프 노드 Rjm에 대해 다음 출력 값을 계산한다.

   ![그림](/image/image-20221024174530916.png)

   리프 노드 Rjm이 하나 이상의 훈련 예제를 포함할 수 있으므로 합산을 포함할 수 있음을 알 수 있다.

   d. 이전 트리에 출력 값 𝛾𝑚을 추가하여 모델을 업데이트한다.

   ![그림](/image/image-20221024174704318.png)

   현재 트리 𝛾𝑚의 전체 예측 값을 이전 트리 𝐹𝑚에 추가하는 대신, 우리는 모델 0과 1 사이의 학습률  0.1에서 1사이의  𝜂 을 스케일링합니다. 과적합을 방지하는 데 도움이 되는 작은 단계를 거쳐 점진적으로 증가한다.



## Illustrating gradient boosting for classification

이전 두 하위 섹션에서는 이진 분류를 위한 그래디언트 부스팅 알고리즘의 압축된 수학적 세부 정보를 살펴보았습니다. 이러한 개념을 더 명확하게 하기 위해 작은 장난감 예,  다음 세 가지 예의 훈련 데이터 세트에 적용해 보겠다.

![그림](/image/image-20221024114337458.png)

루트 노드를 구성하고 로그(홀수)를 계산하는 1단계부터 시작하고, 로그(홀수)를 클래스 구성원 확률로 변환하고 의사 잔차를 계산하는 2a단계부터 시작한다. 3장에서 배운 것을 기반으로 확률은 성공 횟수를 실패 횟수로 나눈 값으로 계산할 수 있습니다. 여기에서 레이블 1을 성공으로 간주하고 레이블 0을 실패로 간주하므로 odd는  2/1로 계산됩니다. 1단계와 2a단계를 수행한 결과는 다음과 같다.



![그림](/image/image-20221024114349807.png)

다음으로 2b단계에서 의사  잔차 r에 새 트리를 맞춘다. 그런 다음 2c 단계에서 그림 7.14와 같이 이 트리에 대한 출력 값 𝛾을 계산한다.



![그림](/image/image-20221024114405377.png)

그런 다음 마지막 2d 단계에서 이전 모델과 현재 모델을 업데이트한다. 학습률이 𝜂 = 0.1라고 가정하면 첫 번째 훈련 예제에 대한 결과 예측이다음 그림과 같이 나타낸다.



![그림](/image/image-20221024114424360.png)



이제 첫 번째 라운드(m = 1)의 2a~2d단계를 완료했으므로 두 번째 라운드(m = 2)에 대해 2a~2d단계를 실행할 수 있다. 두 번째 라운드에서는 다음에서 반환된 log(odds)를 사용합니다. 예를 들어, 2A 단계에 대한 입력인 𝐹1 (𝑥1 ) = 0.839인 업데이트된 모델이다. 두 번째 라운드에서 얻은 새로운 값은 다음과 같다.

![그림](/image/image-20221024114446016.png)

.

 예측 확률이 긍정적 클래스에 대해 더 높고 부정적 클래스에 대해 더 낮다는 것을 알 수 있다. 

- 결과적으로 잔차도 작아지고 있음.
-  M 개의 트리가 적합하거나 잔차가 사용자 지정 임계값보다 작을 때까지 2a~2d 단계의 프로세스가 반복됨.
- 그래디언트 부스팅 알고리즘이 완료되면 3장의 로지스틱 회귀와 같이 0.5에서 최종 모델 FM(x)의 확률 값을 임계값으로 지정하여 클래스 레이블을 예측하는 데 사용할 수 있음.
- 로지스틱 회귀와 대조적으로 , 그래디언트 부스팅은 여러 트리로 구성되며 비선형 결정 경계를 생성함.
