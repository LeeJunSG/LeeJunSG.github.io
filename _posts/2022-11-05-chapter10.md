layout single title: "Chapter 10"


## Working with Unlabeled Data – Clustering Analysis

   

기어를 전환하고 사전에 정답을 알지 못하는 데이터의 숨겨진 구조를 발견할 수 있는 비지도 학습 기술의 범주인 클러스터 분석

-  클러스터링의 목표는 동일한 클러스터의 항목이 다른 클러스터의 항목보다 서로 더 유사하도록 데이터에서 자연스러운 그룹화를 찾음
- 탐색적 특성을 감안할 때 클러스터링은 데이터를 의미 있는 구조로 구성함
  - k-means 알고리즘을 사용하여 유사성 중심 찾기
  - 계층적 클러스터링 트리를 구축하기 위해 상향식 접근 방식
  - 밀도 기반 클러스터링 접근 방식을 사용하여 임의의 개체 모양 식별



## Grouping objects by similarity using k-means

클러스터링 알고리즘 중 하나인 k-means 클러스터링

- 다른 그룹의 개체보다 서로 더 관련이 있는 유사한 개체의 그룹을 찾을 수 있게 해주는 기술

- 클러스터링의 비즈니스 지향 응용 프로그램의 예

  - 문서, 음악 및 영화를 서로 다른 주제별로 그룹화

  - 추천 엔진의 기반으로 공통 구매 행동을 기반으로 유사한 관심사를 공유하는 고객을 찾는 것

    

## k-means clustering using scikit-learn

 k-means 알고리즘

- 구현하기가 매우 쉬움
- 다른 클러스터링 알고리즘에 비해 계산적으로 매우 효율적임
- k-means 알고리즘은 프로토타입 기반 클러스터링 범주에 속함




프로토타입 기반 클러스터링은 각 클러스터가 프로토타입으로 표현되는 것을 의미한다. 

- 프로토타입은 일반적으로 연속적인 특징을 가진 유사한 점의 centroid(평균) 또는 medoid(가장 대표적인 또는 속해 있는 다른 모든 점까지의 거리를 최소화하는 점)임

 k-means는 구형의 클러스터를 식별하는 데 매우 뛰어나지만 이 클러스터링 알고리즘의 단점 중 하나는 클러스터의 수 k를 선험적으로 지정해야 한다는 것이다. 

- k에 대한 부적절한 선택은 클러스터링 성능을 저하시킬 수 있다. 


k-평균 클러스터링의 시각화

![그림](/image/image-20221106011959132.png)

데이터 세트는 2차원 산점도를 통해 시각화된 밀도가 더 높은 3개 영역으로 대략적으로 그룹화된 무작위로 생성된 150개의 점으로 구성

![그림](/image/image-20221106012022544.png)

클러스터링의 실제 적용에서는 이러한 예에 대한 실제 범주 정보가 없다. 

- 클래스 레이블이 주어지면 이 작업은 지도 학습 범주에 속함

  

 k-means 알고리즘을 사용하여 달성할 수 있는 기능 유사성의 4단계

1. 예제에서 무작위로 k개의 중심을 초기 클러스터 중심으로 선택
2. 각 예제를 가장 가까운 중심 𝜇^(𝑗),(𝑗는 {1,…,𝑘}에 속함)에 할당
3. 중심을 할당된 예제의 중심으로 이동
4. 클러스터 할당이 변경되지 않거나 사용자 정의 허용 오차 또는 최대 반복 횟수에 도달할 때까지 2단계와 3단계를 반복



유사도를 거리의 반대로 정의할 수 있다.

- 연속적인 특징이 있는 클러스터링 예제에 일반적으로 사용되는 거리는 m차원 공간에서 두 점 x와 y 사이의 제곱 유클리드 거리

![그림](/image/image-20221106012047657.png)

- 앞의 방정식에서 인덱스 j는 예제 입력 x 및 y의 j번째 차원(특성 열)을 나타냄
- 클러스터 관성이라고도 하는 클러스터 내 SSE(Sum of Squared Error)를 최소화하기 위한 반복적 접근 방식

![그림](/image/image-20221106012111466.png)

여기서 𝝁^(𝑗)은 클러스터 j의 대표점(중심)이다. w^(i, j)는 예제 x(i)가 클러스터 j에 있으면 1이고 그렇지 않으면 0이다.

![그림](/image/image-20221106012116827.png)

간단한 k-means 알고리즘이 작동하는 방식을 배웠으므로 이제 scikit-learn의 클러스터 모듈에서 KMeans 클래스를 사용하여 예제 데이터 세트에 적용했을 때의 그림 :

![그림](/image/image-20221106012245485.png)

k-means가 각 구의 중심에 3개의 중심을 배치한 것을 확인 할 수 있음.



k-means는 클러스터 수 k를 선험적으로 지정해야 하는 단점이 있다. 

- 특히 시각화할 수 없는 더 높은 차원의 데이터 세트로 작업하는 경우 실제 응용 프로그램에서 선택할 클러스터의 수가 항상 명확하지 않을 수 있음
- k-means의 다른 속성은 클러스터가 겹치지 않고 계층적이지 않으며 각 클러스터에 하나 이상의 항목이 있다고 가정함



## A smarter way of placing the initial cluster centroids using k-means++



k-means++ 알고리즘을 통해 초기 중심을 서로 멀리 배치

k-means++의 초기화는 다음과 같이 요약할 수 있습니다.

1. 선택 중인 k 중심을 저장하기 위해 빈 집합 M을 초기화
2. 입력 예제에서 첫 번째 중심 𝝁^(𝑗)을 무작위로 선택하고 M에 할당
3. M에 없는 x(i)의 각 예에 대해 M의 중심에 대한 최소 제곱 거리 d(x(i), M)2를 찾음
4. 다음 중심 𝝁^(𝑝)을 무작위로 선택하려면 다음과 같은 가중 확률 분포를 사용
   ![그림](/image/image-20221106013400699.png)
   - 배열의 모든 점을 수집하고 가중 무작위 샘플링을 선택하여 거리의 제곱이 클수록 점이 중심으로 선택될 가능성이 높아짐
5. k 중심이 선택될 때까지 3단계와 4단계를 반복합니다.
6. 고전적인 k-평균 알고리즘을 진행



## Hard versus soft clustering

하드 클러스터링은 k-means 및 k-means++ 알고리즘에서와 같이 데이터 세트의 각 예제가 정확히 하나의 클러스터에 할당되는 알고리즘 제품군을 설명한다. 

대조적으로, 소프트 클러스터링을 위한 알고리즘은 하나 이상의 클러스터에 예를 할당한다. 

- 소프트 클러스터링의 예
  - 퍼지 C-평균(FCM) 알고리즘(소프트 k-평균 또는 퍼지 k-평균이라고도 함)

FCM 절차는 k-평균과 매우 유사하다. 

- 하드 클러스터 할당을 각 클러스터에 속하는 각 포인트에 대한 확률로 대체함.

k-평균에서 이진 값의 희소 벡터를 사용하여 예제 x의 클러스터 구성원을 표현할 수 있음

![그림](/image/image-20221106013614550.png)

값이 1인 인덱스 위치는 예제가 할당된 클러스터 중심 𝝁^(𝑗)을 나타냅니다(k = 3, 𝑗{1, 2, 3}으로 가정). 대조적으로 FCM의 구성원 벡터는 다음과 같이 나타낼 수 있다.

![그림](/image/image-20221106013636270.png)

여기서 각 값은 [0, 1] 범위에 속하며 각 클러스터 중심의 구성원 확률을 나타낸다. 주어진 예에 대한 구성원의 합은 1과 같다. 



FCM 알고리즘을 4가지 주요 단계

1. k 중심의 수를 지정하고 각 점에 대해 클러스터 구성원을 무작위로 할당합니다.
2. 클러스터 중심을 계산합니다.
3. 각 포인트에 대한 클러스터 멤버십 업데이트
4. 구성원 계수가 변경되지 않거나 사용자 정의 허용오차 또는 최대 반복 횟수에 도달할 때까지 2단계와 3단계를 반복합니다.



FCM의 목적 함수(J(m)는 k-means에서 최소화하는 클러스터 내 SSE와 유사하다.

![그림](/image/image-20221106013659272.png)

- 소속 지표 w(i, j)는 k-means(𝑤^(𝑖,j) ∈ {0, 1} )에서와 같이 이진 값이 아니라 클러스터 소속 확률(𝑤^(𝑖,j) ∈ [0, 1]). 또한 w^(i, j)에 추가 지수를 추가됨
- 지수 m, 1보다 크거나 같은 숫자(일반적으로 m = 2)는 퍼지 정도를 제어하는 소위 퍼지 계수(또는 단순히 퍼지기)임
- m 값이 클수록 클러스터 구성원 w(i, j)가 작아지므로 클러스터가 더 퍼지게 된다. 

클러스터 멤버십 확률 자체는 다음과 같이 계산된다.

![그림](/image/image-20221106013718669.png)

예를 들어, 이전 k-means 예제에서와 같이 3개의 클러스터 센터를 선택한 경우 다음과 같이 𝝁^(𝑗) 클러스터에 속하는 𝒙^(𝑖)의 구성원을 계산할 수 있다.

![그림](/image/image-20221106013742248.png)

클러스터 자체의 중심 𝝁^(𝑗)은 각 예제가 해당 클러스터에 속하는 정도에 따라 가중치가 부여된 모든 예제의 평균으로 계산된다(𝑤^(𝑖,j)^𝑚).

![그림](/image/image-20221106013801940.png)

반면에 FCM은 일반적으로 수렴에 도달하기 위해 더 적은 반복을 필요로 함.



## Using the elbow method to find the optimal number of clusters

비지도 학습의 주요 과제 중 하나는 우리가 확실한 답을 모른다는 것

- 지도 모델의 성능을 평가하기 위해 모델 평가 및 초매개변수 조정을 위한 모범 사례 학습에서 사용한 기술을 적용할 수 있는 데이터 세트에는 실제 클래스 레이블이 없다.

- 클러스터링의 품질을 정량화하려면 클러스터 내 SSE(왜곡)와 같은 고유 메트릭을 사용하여 다양한 k-평균 클러스터링 모델의 성능을 비교해야 함.

  

## Quantifying the quality of clustering via silhouette plots

클러스터링의 품질을 평가하는 또 다른 고유한 메트릭은 실루엣 분석이며, 이 장의 뒷부분에서 논의할 k-평균 이외의 클러스터링 알고리즘에도 적용할 수 있습니다. 실루엣 분석을 그래픽 도구로 사용하여 클러스터의 예가 얼마나 밀접하게 그룹화되었는지 측정할 수 있습니다. 데이터 세트에 있는 단일 예의 실루엣 계수를 계산하기 위해 다음 세 단계를 적용할 수 있습니다.

1. 클러스터 응집력 a(i)를 예제 x(i)와 동일한 클러스터의 다른 모든 점 사이의 평균 거리로 계산합니다.
2. 다음으로 가장 가까운 클러스터에서 클러스터 분리 b(i)를 예제 x(i)와 가장 가까운 클러스터의 모든 예제 사이의 평균 거리로 계산합니다.
3. 다음과 같이 군집 응집력과 분리도의 차이를 둘 중 큰 값으로 나누어 실루엣 s(i)를 계산합니다.

![그림](/image/image-20221106022135541.png)

- 실루엣 계수는 –1에서 1까지의 범위에 속함

- 앞의 방정식을 기반으로 클러스터 분리와 응집력이 같으면 실루엣 계수가 0임을 알 수 있음(b(i) = a(i)). 

- b(i) >> a(i)인 경우 이상적인 실루엣 계수 1에 가까워짐

  

평균(실루엣_샘플(...)) :  k = 3인 k-평균 클러스터링에 대한 실루엣 계수 그림

![그림](/image/image-20221106022429171.png)

위의 실루엣 플롯에서 볼 수 있듯이 실루엣 계수는 0에 가깝지 않고 평균 실루엣 점수와 거의 동일하게 떨어져 있습니다.

- 좋은 클러스터링의 지표
- 클러스터링의 장점을 요약하기 위해 플롯(점선)에 평균 실루엣 계수를 추가



상대적으로 나쁜 클러스터링에 대한 실루엣 플롯이 어떻게 보이는지 확인하기 위해 두 개의 중심만 있는 k-평균 알고리즘의 그림

![그림](/image/image-20221106022626541.png)

![그림](/image/image-20221106022637208.png)





## Organizing clusters as a hierarchical tree

프로토타입 기반 클러스터링에 대한 대안적 접근 방식인 계층적 클러스터링

- 계층적 클러스터링 알고리즘의 한 가지 이점은 덴드로그램(이진 계층적 클러스터링의 시각화)을 플롯할 수 있다는 점

- 의미 있는 분류를 생성하여 결과를 해석하는 데 도움이 됨

- 이 계층적 접근 방식의 또 다른 이점은 클러스터 수를 미리 지정할 필요가 없음

  

계층적 클러스터링에 대한 두 가지 주요 접근 방식:  응집 및 분할 계층적 클러스터링

- 분할 계층적 클러스터링에서는 전체 데이터 세트를 포함하는 하나의 클러스터로 시작하여 각 클러스터에 하나의 예만 포함될 때까지 클러스터를 더 작은 클러스터로 반복적으로 분할
- 응집 클러스터링은 각 예제를 개별 클러스터로 시작하고 클러스터가 하나만 남을 때까지 가장 가까운 클러스터 쌍을 병합



## Grouping clusters in a bottom-up fashion

집합적 계층적 클러스터링을 위한 두 가지 표준 알고리즘은 단일 연결과 완전 연결이다.

- 단일 연결을 사용하여 각 군집 쌍에 대해 가장 유사한 구성원 간의 거리를 계산하고 가장 유사한 구성원 간의 거리가 가장 작은 두 군집을 병합함
- 완전한 연결 접근 방식은 단일 연결과 유사하지만 각 클러스터 쌍에서 가장 유사한 구성원을 비교하는 대신 가장 유사하지 않은 구성원을 비교하여 병합을 수행함 

![그림](/image/image-20221106023357989.png)

계층적 완전한 연결 클러스터링의 반복적인 절차

1. 모든 예의 쌍별 거리 행렬을 계산
2. 각 데이터 포인트를 싱글톤 클러스터로 나타냄
3. 가장 유사하지 않은(먼) 구성원 간의 거리를 기준으로 두 개의 가장 가까운 클러스터를 병합
4. 클러스터 연결 매트릭스를 업데이트
5. 하나의 클러스터가 남을 때까지 2-4단계를 반복



## Performing hierarchical clustering on a distance matrix

덴드로그램은 응집적 계층적 클러스터링 동안 형성된 다양한 클러스터를 요약함

예를 들어, ID_0 및 ID_4, 그 다음으로 ID_1 및 ID_2가 이어지는 예가 유클리드 거리 메트릭을 기반으로 하는 가장 유사한 예임을 알 수 있는 그림 :

![그림](/image/image-20221106025126522.png)



## Attaching dendrograms to a heat map

실제 응용 프로그램에서 계층적 클러스터링 덴드로그램은 열 지도와 함께 사용되는 경우가 많다.

- 데이터 배열 또는 교육 예제가 포함된 행렬의 개별 값을 색상 코드로 나타냄 

덴드로그램을 히트 맵 플롯에 연결한 그림 : .

![그림](/image/image-20221106030947677.png)

히트 맵의 행 순서는 덴드로그램의 예제 클러스터링을 반영한다. 



## Locating regions of high density via DBSCAN



밀도 기반 클러스터링은 밀집된 포인트 영역을 기반으로 클러스터 레이블을 할당한다.

- DBSCAN에서 밀도의 개념은 지정된 반경 𝜀 내의 포인트 수로 정의됨

DBSCAN 알고리즘의 특수 레이블 지정 기준

- 최소한 지정된 수(MinPts)의 인접 포인트가 지정된 반경 𝜀𝜀 내에 있는 경우 포인트는 핵심 포인트로 간주됨

- 경계 점은 𝜀 내에서 MinPts보다 적은 이웃을 갖지만 코어 점의 𝜀 반경 내에 있음

- 핵심 포인트도 경계 포인트도 아닌 다른 모든 포인트는 노이즈 포인트로 간주됨

  

DBSCAN 알고리즘의 단계

1. 각 코어 포인트 또는 코어 포인트의 연결된 그룹에 대해 별도의 클러스터를 형성됨
2. 각 경계점을 해당 핵심점의 클러스터에 할당함

다음 그림은 핵심 포인트, 경계 포인트 및 노이즈 포인트의 요약



![그림](/image/image-20221106031712323.png)

- DBSCAN을 사용하는 주요 이점 중 하나는 클러스터가 k-means에서와 같이 구형 모양을 가지고 있다고 가정하지 않음. 

- DBSCAN은 각 포인트를 클러스터에 할당할 필요는 없지만 노이즈 포인트를 제거할 수 있다는 점에서 k-means 및 계층적 클러스터링과 다름

  

k-평균 클러스터링, 계층적 클러스터링 및 DBSCAN을 비교하기 위해 반달 모양 구조의 새 데이터 세트의 예

- 각각 100개의 예제(데이터 포인트)로 구성된 두 개의 보이는 반달 모양 그룹의 그림 :

![그림](/image/image-20221106031749768.png)

시각화된 클러스터링 결과를 기반으로 k-means 알고리즘이 두 클러스터를 분리할 수 없었고 계층적 클러스터링 알고리즘이 이러한 복잡한 모양에 문제가 있음

![그림](/image/image-20221106031813627.png)

DBSCAN 알고리즘은 DBSCAN의 강점 중 하나인 임의 모양의 클러스터링 데이터를 강조하는 반달 모양을 성공적으로 감지할 수 있음

![그림](/image/image-20221106031834616.png)

DBSCAN의 몇 가지 단점

- 고정된 수의 훈련 예제를 가정할 때 데이터 세트의 기능 수가 증가함에 따라 차원의 저주의 부정적인 영향이 증가
  - 유클리드 거리 측정법을 사용하는 경우
- 차원의 저주 문제는 DBSCAN에만 있는 것이 아니라 유클리드 거리 메트릭을 사용하는 다른 클러스터링 알고리즘(예: k-평균 및 계층적 클러스터링 알고리즘)에도 영향을 미침
- DBSCAN에 두 개의 하이퍼파라미터(MinPts 및 𝜀 )가 있어 좋은 클러스터링 결과를 얻기 위해 최적화해야 함
- 데이터 세트의 밀도 차이가 상대적으로 큰 경우 MinPts와 𝜀의 적절한 조합을 찾는 것이 문제가 됨



데이터가 시각화하기 어렵거나 불가능하게 만드는 여러 차원으로 제공되는 경우 주어진 데이터 세트에서 어떤 클러스터링 알고리즘이 가장 잘 수행되는지 항상 명확하지 않다.

또한 성공적인 클러스터링은 알고리즘과 하이퍼파라미터에만 의존하지 않는다는 점을 강조하는 것이 중요하다.

- 적절한 거리 측정법의 선택과 실험 설정을 안내하는 데 도움이 될 수 있는 도메인 지식의 사용이 훨씬 더 중요할 수 있음 
- 차원의 저주의 맥락에서 클러스터링을 수행하기 전에 차원 축소 기술을 적용하는 것이 일반적임
- 비지도 데이터 세트에 대한 이러한 차원 축소 기술에는 주성분 분석 및 t-SNE가 포함됨
- 데이터 세트를 2차원 부분 공간으로 압축하는 것이 특히 일반적이므로 2차원 산점도를 사용하여 클러스터와 할당된 레이블을 시각화할 수 있으며 이는 결과를 평가하는 데 특히 유용함
